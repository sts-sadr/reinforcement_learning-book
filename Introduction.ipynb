{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Introduction\n",
    "Reinforcement learning consists of 4 elements:\n",
    "- **Policy**: Defines the way the agent should behave.  \n",
    "Mapping from Environment State -> Action.\n",
    "\n",
    "\n",
    "- **Reward Signal**: The Goal of the reinforcement algorithm.  \n",
    "At each step the agent receives a single numeric \"Reward\". The goal of the system is to maximize this reward over the lifetime of the agent.  \n",
    "The reward is calculated based on the last action the agent made\n",
    "\n",
    "\n",
    "- **Value Function**: The expected reward in the long-run.  \n",
    "Opposed to the reward signal which may be good or bad for the immidiate action, the value function estimates the cummulative rewared expected in the long run.  \n",
    "This is important, as some actions may appear not optimal in the immidiate surrounding but provide enormeous value in the long run.  \n",
    " - **Examples**:  \n",
    " -\"Giving\" a piece in chess to hold a better position.  \n",
    " -Training, although it's hard right now, but yields great results later\n",
    " \n",
    " \n",
    "- **Model** (Optional): A Model of the Environment. Allows to infer how the environment will behave based on the agent's actions.  \n",
    "There are *Model-Based* systems and there are *Model Free* systems.  \n",
    "The Model-Free systems are based on Trial & Error learning Vs. Planning for the Model-Based systems.\n",
    "\n",
    "\n",
    "\n",
    "As in most cases, **we cannot compute the optimal solution** There are multiple ways to tackle reinforcement learning, either as an *Optimization* problem where the *value function* is the most important part.  \n",
    "Or with an *Evolutionary* method like Genetic Algorithms to provide a static policy. \n",
    "\n",
    "- Model-Free algorithms are usually good when the environment modeling is hard.  The model-free algorithms doesn't require any knowledge on the environment to learn/evolve.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Self-Play Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?\n",
    "\n",
    "**Answer**: \n",
    "- A Random opponent makes our agent see more states then a directed player, as it can perform \"bad\" moves that a directed player wouldn't have made\n",
    "- When both sides learn, the learning process will be faster as we double the number of examples for the same time.\n",
    "- With directed opponent we expect to still see all or most of the possible actions on the board.  \n",
    "  This because the opponent should have an Exploration / Exploitation notion also and should sometimes explore random moves.\n",
    "\n",
    "Combining all of this observations together, we would expect the algorithm to end up learning the same policy (at least for a this a game of tic-tac-toe).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2**: Symmetries Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n",
    "\n",
    "**Answer**: \n",
    "- In Tic-Tac-Toe the board is square.  Thus:  \n",
    "We can Rotate or mirror the board to match different positions\n",
    "\n",
    "If we will use a \"normalizing\" function to match rotation / mirror states to one another we could reduce our Sample Space / Solution Space by (# Rotations + # mirrors).\n",
    "This means we will:\n",
    "- **Accelerate our learning process** by this factor. \n",
    "- **Reduce our memory requirements**.\n",
    "- **Reduce the look-ahead needed / possible**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.3**: Greedy Play Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?\n",
    "\n",
    "**Answer**:\n",
    "- Depends on the game\n",
    "- A possible problem for a greedy player in chess, given this two options:  \n",
    " - Eat the opponent's queen this turn and then be mated.\n",
    " - Give up your queen this turn and mat your opponent in the next turn.\n",
    " \n",
    " A Greedy player will choose option 1, while a player with a more future-looking value function would choose option 2 which is clearly better.\n",
    " \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.4**: *Learning from Exploration* - Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n",
    "\n",
    "**Answer**:\n",
    "- In this variation we are learning from Exploitation steps and Exploration steps.  *(Opposed to Exploitation steps only)*\n",
    " - Normaly, exploration steps do not update their parent state.  \n",
    " Since $V({S_t}) = V({S_t})+\\alpha \\times V({S_{t+1}})$, If we are currently at state ${S_t}$:  \n",
    "   - **Without taking Exploration into account** *(Normal)*: The step will only be updated when we choose the highest winning probability move.  \n",
    "   This means that we evaluate the *Current* position, based on the *Optimal* moves we can do in the future.\n",
    "   Since we evaluate the *Perceived Optimal* move at each turn, we expect $V(S_t)$ to show *Optimal Value* for the branch.\n",
    "   - **With taking Exploration into account**: The step will be updated whether we choose the optimal move or a random one. \n",
    "   This means that we evaluate the *Current* position, based on the *A* move we can do in the future.\n",
    "   Since we evaluate a *Random* move, we can no longer expect $V(S_t)$ to show *Optimal Value* for the branch.  \n",
    "     - If an optimal move was selected, the updated value will be as normal\n",
    "     - If an under-performing move was selected, it will lower the value of the current state! **Although in reality, it can produce better results**  \n",
    "     This can lead to our algorithm Under-Performing, selecting moves that are not optimal.\n",
    "\n",
    "-----\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Tac Toe\n",
    "## Environment & Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
