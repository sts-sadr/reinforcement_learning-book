{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-arm Bandits\n",
    "## Feedback\n",
    "**Evaluative Feedback**: \n",
    "- In it's pure form, depends only on the action taken\n",
    "- Tells us how good was the action we took\n",
    "- Doesn't tell us which action was best\n",
    "\n",
    "**Instructive Feedback**:  \n",
    "- In it's pure form, independent of the action taken\n",
    "- Tells us which action was best to take\n",
    "- Doesn't indicate how well our action (or any other for that matter) performed\n",
    "- Used in its pure form for Supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-armed Bandit\n",
    "\n",
    "Faced with a choice of K different options / actions.  After each action, you receive an immidiate numerical reward (depends on the action).  \n",
    "The objective is to Maximize the total reward over N time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A_t$: Action selected at time $t$  \n",
    "$R_t$: Reward for $A_t$  \n",
    "$\\large{q_*(a)}$: $\\large{\\mathbb{E}[R_t|A_t=a]}$    //The expected value of reward, given action $a$ is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the problem?    We can just always choose the highest expected reward action!  \n",
    "This is true! When we know $q*(a)$ we can be *greedy* and **exploit** this information and choose the most valueable action.  \n",
    "BUT, In most situations we will not know what is $q*(a)$.  \n",
    "Because we don't know what $q*(a)$ is, we will need to **explore** and increase our certainty about $q*(a)$ for different actions.  (Make $Q_t(a)$ as close to $q*(a)$ as possible)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-Value Methods\n",
    "This methods are used to evalate the true *value* of an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: *Sample Average*\n",
    "The Value of an action is the mean reward from doing that action up to current time.  \n",
    "We can easily formulate it to:\n",
    "$\\Large{Q_t(a) = \\frac{\\sum_{i=1}^{t-1}{R_i * \\mathbb{1}_{A_i=a}}}{\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i=a}}}}$  \n",
    "$\\mathbb{1}_{predicate}$ = 1 if true, else 0  \n",
    "\n",
    "In this equation, $\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i=a}} \\rightarrow \\infty$, $Q_t(a)$ $\\rightarrow$ $q*(a)$  \n",
    "\n",
    "We can couple this equation, with the selection method: $A_t=\\underset{a}{\\arg\\max} {Q_t(a)}$ for a *greedy* selection process\n",
    "\n",
    "#### $\\large\\epsilon-{greedy}$ Selection Method\n",
    "Since we want to support **exploration** factor e for ~ $\\epsilon$ of the times, we can set a rule so that:  \n",
    "$A_t(e)= \\{ \\array{\\underset{a}{\\arg\\max} {Q_t(a)} & with \\ probability & 1-\\epsilon \\\\ Random(a) & with \\ probability & \\epsilon } \\}$  \n",
    "\n",
    "In this case, we know that we **Explore** for $\\epsilon$ of the time, and **Exploit** for $1-\\epsilon$ of the time  \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: In $\\epsilon$-greedy action selection, for the case of two actions and $\\epsilon$ = 0.5, what is the probability that the greedy action is selected?  \n",
    "\n",
    "**Answer**: The greedy action is selected $1-\\epsilon$ of the times, so:  \n",
    "$\\epsilon=0.5 \\| 1-\\epsilon=0.5 \\\\ {Or} \\\\ $  \n",
    "$\\Pr(e|e\\geq0.5) = 0.5 = 50\\%$  \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-armed Testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 31896.887336695825\n",
      "arm 0:\t # chosen: 40\t mean: -6.421229486912091\t true: -6.961145778345863\t delta: 0.5399162914337712\t delta %: 0.08408300817378378\n",
      "\n",
      "arm 1:\t # chosen: 32\t mean: -24.183908519868076\t true: -25.01095101124033\t delta: 0.8270424913722536\t delta %: 0.03419804911570856\n",
      "\n",
      "arm 2:\t # chosen: 107\t mean: 0.6090254975355811\t true: 0.6560828318406078\t delta: 0.04705733430502668\t delta %: 0.07726660787675388\n",
      "\n",
      "arm 3:\t # chosen: 47\t mean: -2.6495736440723805\t true: -2.4598047298925043\t delta: 0.18976891417987618\t delta %: 0.07162243427519999\n",
      "\n",
      "arm 4:\t # chosen: 1779\t mean: 18.54243160305969\t true: 18.466928280679273\t delta: 0.0755033223804169\t delta %: 0.004071921309822035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class bandit_arm():\n",
    "    def __init__(self, mean: float, std: float):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.distribution = np.random.normal\n",
    "        \n",
    "    def r(self):\n",
    "        while True:\n",
    "            yield self.distribution(loc=self.mean, \n",
    "                                    scale=self.std)\n",
    "\n",
    "class bandit():\n",
    "    def __init__(self, k: int, eps: float):\n",
    "        self.A = self._create_arms(k)\n",
    "        self.R = dict()\n",
    "        self.epsilon = eps\n",
    "        self.total_reward = 0\n",
    "        self.cum_rewards = []\n",
    "        \n",
    "    def game(self, T: int):\n",
    "        self.R = self._init_rewards()\n",
    "        self.total_reward = 0\n",
    "        for i in range(T):\n",
    "            Rt = self.play()\n",
    "            self.total_reward += Rt\n",
    "            self.cum_rewards.append(self.total_reward)\n",
    "        return self.total_reward\n",
    "            \n",
    "    def play(self):\n",
    "        a = self._choose_action()\n",
    "        return self._do_action(a)\n",
    "        \n",
    "    def _create_arms(self, k: int):\n",
    "        return [bandit_arm(np.random.normal(loc=0, scale=20), abs(np.random.normal(loc=0, scale=2))) for a in range(k)]\n",
    "    \n",
    "    def _init_rewards(self):\n",
    "        rewards = dict()\n",
    "        for arm in range(len(self.A)):\n",
    "            rewards[arm] = np.zeros(1)\n",
    "        return rewards\n",
    "        \n",
    "    def _Q(self, Ra: np.array):\n",
    "        def _sample_average(Ra: np.array):\n",
    "            mu = Ra.mean()\n",
    "            return mu\n",
    "    \n",
    "        value_function = _sample_average\n",
    "        return value_function(Ra)\n",
    "    \n",
    "    def _choose_action(self):\n",
    "        r = np.random.uniform()\n",
    "        if r > self.epsilon:\n",
    "            Qt_with_indexes = [(b._Q(b.R[k]), k) for k in b.R.keys()]\n",
    "            Qt = [r[0] for r in Qt_with_indexes]\n",
    "            if Qt:\n",
    "                chosen_arm =  np.argmax(Qt)\n",
    "                chosen_arm = Qt_with_indexes[chosen_arm][1]\n",
    "            else:\n",
    "                chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        else:\n",
    "            chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        return chosen_arm\n",
    "    \n",
    "    def _do_action(self, a):\n",
    "        Rt = next(self.A[a].r())\n",
    "        c = self.R.setdefault(a, np.array([]))\n",
    "        self.R[a] = np.append(self.R[a], Rt)\n",
    "        return Rt\n",
    "\n",
    "k = 5\n",
    "epsilon = 0.1\n",
    "b = bandit(k, epsilon)\n",
    "T = 2000\n",
    "total_reward = b.game(T)\n",
    "print(f'Total Reward: {total_reward}')\n",
    "for i in b.R:\n",
    "    print(f'arm {i}:\\t # chosen: {len(b.R[i])}\\t mean: {np.mean(b.R[i])}\\t true: {b.A[i].mean}\\t delta: {abs(b.A[i].mean-np.mean(b.R[i]))}\\t delta %: {abs(1-b.A[i].mean/np.mean(b.R[i]))}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Exercise 2.2**: Bandit example Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using \"-greedy action selection, sample-average action-value estimates, and initial estimates of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A1 = 1, R1 =\u00001,A2 =2,R2 =1,A3 =2,R3 =\u00002,A4 =2,R4 =2,A5 =3,R5 =0. Onsome of these time steps the \" case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?\n",
    "\n",
    "**Answer**:\n",
    "According to the definition we have: $k=4$, Using Sample-Average and 0 initialization for $Q_1(a)$  \n",
    "Lets track the algorithm:\n",
    "- A1 = 1 / R1 = -1  # Must be random, all $Q_i(a)=0$\n",
    "- A2 = 2 / R2 = 1   # Random, but only between $Q_i(a)\\ where\\ i \\neq 1$\n",
    "- A3 = 2 / R3 = -2  # Greedy action, $Q_2(A_2) = 1$\n",
    "- A4 = 2 / R4 =2    # Random, $Q_3(A_2) = -0.5$ while $Q_3(A_3)\\ \\& \\ Q_3(A_4) = 0$\n",
    "- A5 = 3 / R5 =0    # Random / Greedy-Random betwen $A_3\\ \\& \\ A_4$\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**: In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.\n",
    "\n",
    "**Answer**: Since we are being asked on the long run, we will simplify and assume that the algorithm always chosses the optimal action when not in $\\epsilon$ (Exploratory) mode.  \n",
    "Now, let's look at the algorithms:  \n",
    "\n",
    "The difference between them is the $\\epsilon$, And we will look at an example of:  \n",
    "- $T: 1000$\n",
    "- $RL1: \\epsilon=0.1$ \n",
    "- $RL2: \\epsilon=0.3$\n",
    "- Notations:\n",
    "  - $\\mu(a^*)$: Mean of optimal action\n",
    "  - $\\mu(a)$: Mean for not-optimal action\n",
    "\n",
    "So, what will be the difference in cummulative rewards between 1 and 2?\n",
    "\n",
    "$ T \\times [(1-\\epsilon_1)\\mu(a^*) + \\epsilon_1\\mu(a)] - [(1-\\epsilon_2)\\mu (a^*) + \\epsilon_2\\mu(a)] = $  \n",
    "$= T \\times [(\\epsilon_2 - \\epsilon_1)\\mu(a^*) + (\\epsilon_1 - \\epsilon_2)\\mu(a)] =$  \n",
    "$= T \\times [(\\epsilon_2 - \\epsilon_1)\\mu(a^*) - (\\epsilon_2 - \\epsilon_1)\\mu(a)] =$  \n",
    "$= T \\times [(\\epsilon_2 - \\epsilon_1)\\times(\\mu(a^*) - \\mu(a))] =$  \n",
    "$= T \\times \\Delta\\epsilon\\times\\Delta\\mu =$  \n",
    "$= 1000 \\times 0.2\\times(\\mu(a^*)-\\mu(a)) = 200 \\times \\Delta\\mu $\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
