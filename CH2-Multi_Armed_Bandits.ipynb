{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-arm Bandits\n",
    "## Feedback\n",
    "**Evaluative Feedback**: \n",
    "- In it's pure form, depends only on the action taken\n",
    "- Tells us how good was the action we took\n",
    "- Doesn't tell us which action was best\n",
    "\n",
    "**Instructive Feedback**:  \n",
    "- In it's pure form, independent of the action taken\n",
    "- Tells us which action was best to take\n",
    "- Doesn't indicate how well our action (or any other for that matter) performed\n",
    "- Used in its pure form for Supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-armed Bandit\n",
    "\n",
    "Faced with a choice of K different options / actions.  After each action, you receive an immidiate numerical reward (depends on the action).  \n",
    "The objective is to Maximize the total reward over N time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A_t$: Action selected at time $t$  \n",
    "$R_t$: Reward for $A_t$  \n",
    "$\\large{q_*(a)}$: $\\large{\\mathbb{E}[R_t|A_t=a]}$    //The expected value of reward, given action $a$ is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the problem?    We can just always choose the highest expected reward action!  \n",
    "This is true! When we know $q*(a)$ we can be *greedy* and **exploit** this information and choose the most valueable action.  \n",
    "BUT, In most situations we will not know what is $q*(a)$.  \n",
    "Because we don't know what $q*(a)$ is, we will need to **explore** and increase our certainty about $q*(a)$ for different actions.  (Make $Q_t(a)$ as close to $q*(a)$ as possible)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-Value Methods\n",
    "This methods are used to evalate the true *value* of an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: *Sample Average*\n",
    "The Value of an action is the mean reward from doing that action up to current time.  \n",
    "We can easily formulate it to:\n",
    "$\\Large{Q_t(a) = \\frac{\\sum_{i=1}^{t-1}{R_i * \\mathbb{1}_{A_i=a}}}{\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i=a}}}}$  \n",
    "$\\mathbb{1}_{predicate}$ = 1 if true, else 0  \n",
    "\n",
    "In this equation, $\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i=a}} \\rightarrow \\infty$, $Q_t(a)$ $\\rightarrow$ $q*(a)$  \n",
    "\n",
    "We can couple this equation, with the selection method: $A_t=\\underset{a}{\\arg\\max} {Q_t(a)}$ for a *greedy* selection process\n",
    "\n",
    "#### $\\large\\epsilon-{greedy}$ Selection Method\n",
    "Since we want to support **exploration** factor e for ~ $\\epsilon$ of the times, we can set a rule so that:  \n",
    "$A_t(e)= \\{ \\array{\\underset{a}{\\arg\\max} {Q_t(a)} & with \\ probability & 1-\\epsilon \\\\ Random(a) & with \\ probability & \\epsilon } \\}$  \n",
    "\n",
    "In this case, we know that we **Explore** for $\\epsilon$ of the time, and **Exploit** for $1-\\epsilon$ of the time  \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: In $\\epsilon$-greedy action selection, for the case of two actions and $\\epsilon$ = 0.5, what is the probability that the greedy action is selected?  \n",
    "\n",
    "**Answer**: The greedy action is selected $1-\\epsilon$ of the times, so:  \n",
    "$\\epsilon=0.5 \\| 1-\\epsilon=0.5 \\\\ {Or} \\\\ $  \n",
    "$\\Pr(e|e\\geq0.5) = 0.5 = 50\\%$  \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-armed Testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Arm Bandit with Incremental-Sample-Average Q Function\n",
      "Total Reward: 56868.89407404348\n",
      "Optimal Reward: 29.831098163498734\tOptimal Total Reward: 59662.196326997466\n",
      "Regret: 2793.3022529539885 / 4.68%\n",
      "arm 0:\t # chosen: 28\t mean: 21.46627169306495\t true: 22.257603006666564\t delta: 0.7913313136016136\t delta %: 0.03686393822441314\n",
      "\n",
      "arm 1:\t # chosen: 38\t mean: 18.834840095390188\t true: 18.27824772391524\t delta: 0.5565923714749488\t delta %: 0.029551212999741616\n",
      "\n",
      "arm 2:\t # chosen: 38\t mean: 5.550243373440515\t true: 5.624291919128579\t delta: 0.07404854568806396\t delta %: 0.013341495265308145\n",
      "\n",
      "arm 3:\t # chosen: 1846\t mean: 29.845917913497832\t true: 29.831098163498734\t delta: 0.01481974999909852\t delta %: 0.0004965419405779103\n",
      "\n",
      "arm 4:\t # chosen: 55\t mean: 4.466196845545515\t true: 4.371649707639574\t delta: 0.09454713790594127\t delta %: 0.021169496369207397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class bandit_arm_v1():\n",
    "    def __init__(self, mean: float, std: float):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.distribution = np.random.normal\n",
    "        \n",
    "    def r(self):\n",
    "        while True:\n",
    "            yield self.distribution(loc=self.mean, \n",
    "                                    scale=self.std)\n",
    "\n",
    "class bandit_v1():\n",
    "    def __init__(self, k: int, eps: float):\n",
    "        self.A = self._create_arms(k)\n",
    "        self.R = dict()\n",
    "        self.epsilon = eps\n",
    "        self.total_reward = 0\n",
    "        self.cum_rewards = []\n",
    "        \n",
    "    def game(self, T: int):\n",
    "        self.R = self._init_rewards()\n",
    "        self.total_reward = 0\n",
    "        for i in range(T):\n",
    "            Rt = self.play()\n",
    "            self.total_reward += Rt\n",
    "            self.cum_rewards.append(self.total_reward)\n",
    "        return self.total_reward\n",
    "            \n",
    "    def play(self):\n",
    "        a = self._choose_action()\n",
    "        return self._do_action(a)\n",
    "        \n",
    "    def _create_arms(self, k: int):\n",
    "        return [bandit_arm_v1(np.random.normal(loc=0, scale=20), abs(np.random.normal(loc=0, scale=2))) for a in range(k)]\n",
    "    \n",
    "    def _init_rewards(self):\n",
    "        rewards = dict()\n",
    "        for arm in range(len(self.A)):\n",
    "            rewards[arm] = np.zeros(1)\n",
    "        return rewards\n",
    "        \n",
    "    def _Q(self, Ra: np.array):\n",
    "        def _sample_average(Ra: np.array):\n",
    "            mu = Ra.mean()\n",
    "            return mu\n",
    "    \n",
    "        value_function = _sample_average\n",
    "        return value_function(Ra)\n",
    "    \n",
    "    def _choose_action(self):\n",
    "        r = np.random.uniform()\n",
    "        if r > self.epsilon:\n",
    "            Qt_with_indexes = [(self._Q(self.R[k]), k) for k in b.R.keys()]\n",
    "            Qt = [r[0] for r in Qt_with_indexes]\n",
    "            if Qt:\n",
    "                chosen_arm =  np.argmax(Qt)\n",
    "                chosen_arm = Qt_with_indexes[chosen_arm][1]\n",
    "            else:\n",
    "                chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        else:\n",
    "            chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        return chosen_arm\n",
    "    \n",
    "    def _do_action(self, a):\n",
    "        Rt = next(self.A[a].r())\n",
    "        c = self.R.setdefault(a, np.array([]))\n",
    "        self.R[a] = np.append(self.R[a], Rt)\n",
    "        return Rt\n",
    "\n",
    "k = 5\n",
    "epsilon = 0.1\n",
    "b1 = bandit_v1(k, epsilon)\n",
    "T = 2000\n",
    "total_reward = b1.game(T)\n",
    "optimal_reward = np.max(list(map(lambda x: x.mean, b1.A)))\n",
    "total_optimal_reward = T * optimal_reward\n",
    "print('K-Arm Bandit with Incremental-Sample-Average Q Function')\n",
    "print(f'Total Reward: {total_reward}')\n",
    "print(f'Optimal Reward: {optimal_reward}\\tOptimal Total Reward: {total_optimal_reward}')\n",
    "print(f'Regret: {total_optimal_reward - total_reward} / {100*(1-(total_reward/total_optimal_reward)):.2f}%')\n",
    "for i in b1.R:\n",
    "    print(f'arm {i}:\\t # chosen: {len(b1.R[i])}\\t mean: {np.mean(b1.R[i])}\\t true: {b1.A[i].mean}\\t delta: {abs(b1.A[i].mean-np.mean(b1.R[i]))}\\t delta %: {abs(1-b1.A[i].mean/np.mean(b1.R[i]))}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Exercise 2.2**: Bandit example Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using \"-greedy action selection, sample-average action-value estimates, and initial estimates of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A1 = 1, R1 =\u00001,A2 =2,R2 =1,A3 =2,R3 =\u00002,A4 =2,R4 =2,A5 =3,R5 =0. Onsome of these time steps the \" case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?\n",
    "\n",
    "**Answer**:\n",
    "According to the definition we have: $k=4$, Using Sample-Average and 0 initialization for $Q_1(a)$  \n",
    "Lets track the algorithm:\n",
    "- A1 = 1 / R1 = -1  # Must be random, all $Q_i(a)=0$\n",
    "- A2 = 2 / R2 = 1   # Random, but only between $Q_i(a)\\ where\\ i \\neq 1$\n",
    "- A3 = 2 / R3 = -2  # Greedy action, $Q_2(A_2) = 1$\n",
    "- A4 = 2 / R4 =2    # Random, $Q_3(A_2) = -0.5$ while $Q_3(A_3)\\ \\& \\ Q_3(A_4) = 0$\n",
    "- A5 = 3 / R5 =0    # Random / Greedy-Random betwen $A_3\\ \\& \\ A_4$\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**: In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.\n",
    "\n",
    "**Answer**: Since we are being asked on the long run, we will simplify and assume that the algorithm always chosses the optimal action when not in $\\epsilon$ (Exploratory) mode.  \n",
    "Now, let's look at the algorithms:  \n",
    "\n",
    "The difference between them is the $\\epsilon$, And we will look at an example of:  \n",
    "- $T: 1000$\n",
    "- $RL1: \\epsilon=0.1$ \n",
    "- $RL2: \\epsilon=0.3$\n",
    "- Notations:\n",
    "  - $\\mu(a^*)$: Mean of optimal action\n",
    "  - $\\mu(a)$: Mean for not-optimal actions\n",
    "\n",
    "So, what will be the difference in cummulative rewards between 1 and 2?\n",
    "\n",
    "$ T \\times [(1-\\epsilon_1)\\mu(a^*) + \\epsilon_1\\mu(a)] - [(1-\\epsilon_2)\\mu (a^*) + \\epsilon_2\\mu(a)] = $  \n",
    "$= T \\times [(\\epsilon_2 - \\epsilon_1)\\mu(a^*) + (\\epsilon_1 - \\epsilon_2)\\mu(a)] =$  \n",
    "$= T \\times [(\\epsilon_2 - \\epsilon_1)\\mu(a^*) - (\\epsilon_2 - \\epsilon_1)\\mu(a)] =$  \n",
    "$= T \\times [(\\epsilon_2 - \\epsilon_1)\\times(\\mu(a^*) - \\mu(a))] =$  \n",
    "$= T \\times \\Delta\\epsilon\\times\\Delta\\mu =$  \n",
    "$= 1000 \\times 0.2\\times(\\mu(a^*)-\\mu(a)) = 200 \\times \\Delta\\mu $\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1.5: Incremental Sample-Average  \n",
    "\n",
    "This is a computational imporvement over the last algortihm.  \n",
    "It uses a mathematical \"game\" to calculate the sample-average using 2 variables per arm only (instead of saving the whole reward history).  \n",
    "\n",
    "In the basic Sample-Average we use the value function:  \n",
    "$\\Large{Q_t(a) = \\frac{\\sum_{i=1}^{t-1}{R_i * \\mathbb{1}_{A_i=a}}}{\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i=a}}}}$  \n",
    "\n",
    "Which as we saw is very costly as we need to calculate $\\sum_{i=1}^{t-1}{R_i * \\mathbb{1}_{A_i=a}}$ at every timestamp.  \n",
    "Instead, we can calculate only ${NewEstimate}\\leftarrow{OldEstimate} + {StepSize}[{Target}-{OldEstimate}]$.  \n",
    "Or in our terms: $Q_{t+1} = Q_t + \\alpha[R_t - Q_t]$ where $\\alpha=\\frac{1}{n}$.  \n",
    "\n",
    "\n",
    "In this version all we need to keep at each timestamp is only $Q_t$ and use our $\\alpha$ and current $R_t$ to complete the update which takes *StepSize ($\\alpha$)* towards *Target ($R_t$)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Arm Bandit with Incremental-Sample-Average Q Function\n",
      "Total Reward: 5767.017962052945\n",
      "Optimal Reward: 4.489276407988331\tOptimal Total Reward: 8978.552815976664\n",
      "Regret: 3211.5348539237184 / 35.77%\n",
      "arm 0:\t # chosen: 44\t mean: -47.353454967829975\t true: -45.93386327226607\t delta: 1.419591695563902\t delta %: 0.029978629785900823\n",
      "\n",
      "arm 1:\t # chosen: 33\t mean: -2.498193355140872\t true: -5.517951172730085\t delta: 3.0197578175892135\t delta %: 1.2087766590904772\n",
      "\n",
      "arm 2:\t # chosen: 39\t mean: -1.256194618545976\t true: -7.617885275797883\t delta: 6.361690657251907\t delta %: 5.064255620371513\n",
      "\n",
      "arm 3:\t # chosen: 1849\t mean: 4.457379149200345\t true: 4.489276407988331\t delta: 0.03189725878798644\t delta %: 0.007156056893591556\n",
      "\n",
      "arm 4:\t # chosen: 35\t mean: 0.0694683951320985\t true: 0.6171842619547361\t delta: 0.5477158668226376\t delta %: 7.884389236013321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "class bandit_arm_v2():\n",
    "    def __init__(self, mean: float, std: float):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.distribution = np.random.normal\n",
    "        \n",
    "    def r(self):\n",
    "        while True:\n",
    "            yield self.distribution(loc=self.mean, \n",
    "                                    scale=self.std)\n",
    "\n",
    "class bandit_v2():\n",
    "    def __init__(self, k: int, eps: float):\n",
    "        self.A = self._create_arms(k)\n",
    "        self.R = dict()\n",
    "        self.epsilon = eps\n",
    "        self.n = 0\n",
    "        self.chosen = np.zeros(k)\n",
    "        self.total_reward = 0\n",
    "        self.cum_rewards = []\n",
    "        \n",
    "    def game(self, T: int):\n",
    "        self.R = self._init_rewards()\n",
    "        self.total_reward = 0\n",
    "        self.n = 0\n",
    "        for i in range(T):\n",
    "            self.n += 1\n",
    "            Rt = self.play()\n",
    "            self.total_reward += Rt\n",
    "            self.cum_rewards.append(self.total_reward)\n",
    "        return self.total_reward\n",
    "            \n",
    "    def play(self):\n",
    "        a = self._choose_action()\n",
    "        return self._do_action(a)\n",
    "        \n",
    "    def _create_arms(self, k: int):\n",
    "        return [bandit_arm_v2(np.random.normal(loc=0, scale=20), abs(np.random.normal(loc=0, scale=2))) for a in range(k)]\n",
    "    \n",
    "    def _init_rewards(self):\n",
    "        rewards = dict()\n",
    "        for arm in range(len(self.A)):\n",
    "            rewards[arm] = 0\n",
    "        return rewards\n",
    "    \n",
    "    def _choose_action(self):\n",
    "        r = np.random.uniform()\n",
    "        if r > self.epsilon:\n",
    "            Qt_with_indexes = [(self.R[k], k) for k in self.R.keys()]\n",
    "            Qt = [r[0] for r in Qt_with_indexes]\n",
    "            if Qt:\n",
    "                chosen_arm =  np.argmax(Qt)\n",
    "                chosen_arm = Qt_with_indexes[chosen_arm][1]\n",
    "            else:\n",
    "                chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        else:\n",
    "            chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        return chosen_arm\n",
    "    \n",
    "    def _do_action(self, a):\n",
    "        Rt = next(self.A[a].r())\n",
    "        alpha = (1/self.n)\n",
    "        self.R[a] += alpha * (Rt - self.R[a])\n",
    "        self.chosen[a] += 1\n",
    "        return Rt\n",
    "\n",
    "k = 5\n",
    "epsilon = 0.1\n",
    "b2 = bandit_v2(k, epsilon)\n",
    "T = 2000\n",
    "total_reward = b2.game(T)\n",
    "optimal_reward = np.max(list(map(lambda x: x.mean, b2.A)))\n",
    "total_optimal_reward = T * optimal_reward\n",
    "print('K-Arm Bandit with Incremental-Sample-Average Q Function')\n",
    "print(f'Total Reward: {total_reward}')\n",
    "print(f'Optimal Reward: {optimal_reward}\\tOptimal Total Reward: {total_optimal_reward}')\n",
    "print(f'Regret: {total_optimal_reward - total_reward} / {100*(1-(total_reward/total_optimal_reward)):.2f}%')\n",
    "for i in b2.R:\n",
    "    print(f'arm {i}:\\t # chosen: {b2.chosen[i]:0.0f}\\t mean: {b2.R[i]}\\t true: {b2.A[i].mean}\\t delta: {abs(b2.A[i].mean-b2.R[i])}\\t delta %: {abs(1-b2.A[i].mean/np.mean(b2.R[i]))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing Sample-Average bandit over 2000 turns\n",
      "112 ms ± 3.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f'Timing Sample-Average bandit over {T} turns')\n",
    "%timeit (b1.game(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing Incremental-Sample-Average bandit over 2000 turns\n",
      "29 ms ± 580 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f'Timing Incremental-Sample-Average bandit over {T} turns')\n",
    "%timeit (b2.game(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Stationary Methods  \n",
    "In the past algorithms we assumed that the Reward distribution in each of the arms stays the same throughout the process life.  \n",
    "However, in many cases and mostly in real life, the Reward is **Non-Stationary (Dynamic - Changes with time)**.  \n",
    "\n",
    "To accomodate for the Reward distribution change, we need to give our latest rewards higher value in our Q (value) function.   \n",
    "This will enable the algorithm to both remember its past rewards while taking into account the present situation more carefuly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimal change we can make to our current bandit algorithm to make it Non-Stationary is simply to use a **Fixed** $\\large\\alpha$\n",
    "\n",
    "So, If we use our latest example, the update rule $Q_{t+1}\\leftarrow Q_t + \\alpha[R_t - Q_t]$ with $\\alpha=\\frac{1}{n}$.  This time, we use a fixed $\\alpha\\in(0, 1]$.  \n",
    "\n",
    "The transition to a fixed $\\alpha$ leads to the following update equation:\n",
    "$Q_{n+1} = (1-\\alpha)^nQ_1 + \\sum_{i=1}^n{\\alpha(1-\\alpha)^{n-i}R_i}$.\n",
    "\n",
    "We call this equation a **Weighted Average** because:\n",
    "- The sum $(1-\\alpha)^n + \\sum_{i=1}^n{\\alpha(1-\\alpha)^{n-i}} = 1$\n",
    "\n",
    "We will also note that $\\alpha(1-\\alpha)^{n-i}$ on $R_i$ increases as we get closer to $n$, **giving rescent rewards higher weight then earlier ones.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
