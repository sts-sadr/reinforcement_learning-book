{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-arm Bandits\n",
    "## Feedback\n",
    "**Evaluative Feedback**: \n",
    "- In it's pure form, depends only on the action taken\n",
    "- Tells us how good was the action we took\n",
    "- Doesn't tell us which action was best\n",
    "\n",
    "**Instructive Feedback**:  \n",
    "- In it's pure form, independent of the action taken\n",
    "- Tells us which action was best to take\n",
    "- Doesn't indicate how well our action (or any other for that matter) performed\n",
    "- Used in its pure form for Supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-armed Bandit\n",
    "\n",
    "Faced with a choice of K different options / actions.  After each action, you receive an immidiate numerical reward (depends on the action).  \n",
    "The objective is to Maximize the total reward over N time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A_t$: Action selected at time $t$  \n",
    "$R_t$: Reward for $A_t$  \n",
    "$\\large{q_*(a)}$: $\\large{\\mathbb{E}[R_t|A_t=a]}$    //The expected value of reward, given action $a$ is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the problem?    We can just always choose the highest expected reward action!  \n",
    "This is true! When we know $q*(a)$ we can be *greedy* and **exploit** this information and choose the most valueable action.  \n",
    "BUT, In most situations we will not know what is $q*(a)$.  \n",
    "Because we don't know what $q*(a)$ is, we will need to **explore** and increase our certainty about $q*(a)$ for different actions.  (Make $Q_t(a)$ as close to $q*(a)$ as possible)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-Value Methods\n",
    "This methods are used to evalate the true *value* of an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: *Sample Average*\n",
    "The Value of an action is the mean reward from doing that action up to current time.  \n",
    "We can easily formulate it to:\n",
    "$\\Large{Q_t(a) = \\frac{\\sum_{i=1}^{t-1}{R_i * \\mathbb{1}_{A_i=a}}}{\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i=a}}}}$  \n",
    "$\\mathbb{1}_{predicate}$ = 1 if true, else 0  \n",
    "\n",
    "In this equation, $\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i=a}} \\rightarrow \\infty$, $Q_t(a)$ $\\rightarrow$ $q*(a)$  \n",
    "\n",
    "We can couple this equation, with the selection method: $A_t=\\underset{a}{\\arg\\max} {Q_t(a)}$ for a *greedy* selection process\n",
    "\n",
    "#### $\\large\\epsilon-{greedy}$ Selection Method\n",
    "Since we want to support **exploration** factor e for ~ $\\epsilon$ of the times, we can set a rule so that:  \n",
    "$A_t(e)= \\{ \\array{\\underset{a}{\\arg\\max} {Q_t(a)} & with \\ probability & 1-\\epsilon \\\\ Random(a) & with \\ probability & \\epsilon } \\}$  \n",
    "\n",
    "In this case, we know that we **Explore** for $\\epsilon$ of the time, and **Exploit** for $1-\\epsilon$ of the time  \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: In $\\epsilon$-greedy action selection, for the case of two actions and $\\epsilon$ = 0.5, what is the probability that the greedy action is selected?  \n",
    "\n",
    "**Answer**: The greedy action is selected $1-\\epsilon$ of the times, so:  \n",
    "$\\epsilon=0.5 \\| 1-\\epsilon=0.5 \\\\ {Or} \\\\ $  \n",
    "$\\Pr(e|e\\geq0.5) = 0.5 = 50\\%$  \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-armed Testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Arm Bandit with Incremental-Sample-Average Q Function\n",
      "Total Reward: 31379.996035619253\n",
      "Optimal Reward: 17.763630604770782\tOptimal Total Reward: 35527.26120954156\n",
      "Regret: 4147.265173922307 / 11.67%\n",
      "arm 0:\t # chosen: 41\t mean: -7.126444136058912\t true: -7.3419678055252575\t delta: 0.2155236694663456\t delta %: 0.0302428062791964\n",
      "\n",
      "arm 1:\t # chosen: 128\t mean: 13.851365511335352\t true: 14.082527636262604\t delta: 0.23116212492725197\t delta %: 0.016688760739010045\n",
      "\n",
      "arm 2:\t # chosen: 37\t mean: -15.844734872624295\t true: -16.220333855699085\t delta: 0.37559898307478967\t delta %: 0.02370497115251391\n",
      "\n",
      "arm 3:\t # chosen: 1761\t mean: 17.747797057488857\t true: 17.763630604770782\t delta: 0.01583354728192532\t delta %: 0.0008921415559710599\n",
      "\n",
      "arm 4:\t # chosen: 38\t mean: -20.221314952739434\t true: -20.79069541142129\t delta: 0.5693804586818558\t delta %: 0.028157439811040685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class bandit_arm_v1():\n",
    "    def __init__(self, mean: float, std: float):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.distribution = np.random.normal\n",
    "        \n",
    "    def r(self):\n",
    "        while True:\n",
    "            yield self.distribution(loc=self.mean, \n",
    "                                    scale=self.std)\n",
    "\n",
    "class bandit_v1():\n",
    "    def __init__(self, k: int, eps: float):\n",
    "        self.A = self._create_arms(k)\n",
    "        self.R = dict()\n",
    "        self.epsilon = eps\n",
    "        self.total_reward = 0\n",
    "        self.cum_rewards = []\n",
    "        \n",
    "    def game(self, T: int):\n",
    "        self.R = self._init_rewards()\n",
    "        self.total_reward = 0\n",
    "        for i in range(T):\n",
    "            Rt = self.play()\n",
    "            self.total_reward += Rt\n",
    "            self.cum_rewards.append(self.total_reward)\n",
    "        return self.total_reward\n",
    "            \n",
    "    def play(self):\n",
    "        a = self._choose_action()\n",
    "        return self._do_action(a)\n",
    "        \n",
    "    def _create_arms(self, k: int):\n",
    "        return [bandit_arm_v1(np.random.normal(loc=0, scale=20), abs(np.random.normal(loc=0, scale=2))) for a in range(k)]\n",
    "    \n",
    "    def _init_rewards(self):\n",
    "        rewards = dict()\n",
    "        for arm in range(len(self.A)):\n",
    "            rewards[arm] = np.zeros(1)\n",
    "        return rewards\n",
    "        \n",
    "    def _Q(self, Ra: np.array):\n",
    "        def _sample_average(Ra: np.array):\n",
    "            mu = Ra.mean()\n",
    "            return mu\n",
    "    \n",
    "        value_function = _sample_average\n",
    "        return value_function(Ra)\n",
    "    \n",
    "    def _choose_action(self):\n",
    "        r = np.random.uniform()\n",
    "        if r > self.epsilon:\n",
    "            Qt_with_indexes = [(self._Q(self.R[k]), k) for k in self.R.keys()]\n",
    "            Qt = [r[0] for r in Qt_with_indexes]\n",
    "            if Qt:\n",
    "                chosen_arm =  np.argmax(Qt)\n",
    "                chosen_arm = Qt_with_indexes[chosen_arm][1]\n",
    "            else:\n",
    "                chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        else:\n",
    "            chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        return chosen_arm\n",
    "    \n",
    "    def _do_action(self, a):\n",
    "        Rt = next(self.A[a].r())\n",
    "        c = self.R.setdefault(a, np.array([]))\n",
    "        self.R[a] = np.append(self.R[a], Rt)\n",
    "        return Rt\n",
    "\n",
    "k = 5\n",
    "epsilon = 0.1\n",
    "b1 = bandit_v1(k, epsilon)\n",
    "T = 2000\n",
    "total_reward = b1.game(T)\n",
    "optimal_reward = np.max(list(map(lambda x: x.mean, b1.A)))\n",
    "total_optimal_reward = T * optimal_reward\n",
    "print('K-Arm Bandit with Incremental-Sample-Average Q Function')\n",
    "print(f'Total Reward: {total_reward}')\n",
    "print(f'Optimal Reward: {optimal_reward}\\tOptimal Total Reward: {total_optimal_reward}')\n",
    "print(f'Regret: {total_optimal_reward - total_reward} / {100*(1-(total_reward/total_optimal_reward)):.2f}%')\n",
    "for i in b1.R:\n",
    "    print(f'arm {i}:\\t # chosen: {len(b1.R[i])}\\t mean: {np.mean(b1.R[i])}\\t true: {b1.A[i].mean}\\t delta: {abs(b1.A[i].mean-np.mean(b1.R[i]))}\\t delta %: {abs(1-b1.A[i].mean/np.mean(b1.R[i]))}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Exercise 2.2**: Bandit example Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using \"-greedy action selection, sample-average action-value estimates, and initial estimates of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A1 = 1, R1 =\u00001,A2 =2,R2 =1,A3 =2,R3 =\u00002,A4 =2,R4 =2,A5 =3,R5 =0. Onsome of these time steps the \" case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?\n",
    "\n",
    "**Answer**:\n",
    "According to the definition we have: $k=4$, Using Sample-Average and 0 initialization for $Q_1(a)$  \n",
    "Lets track the algorithm:\n",
    "- A1 = 1 / R1 = -1  # Must be random, all $Q_i(a)=0$\n",
    "- A2 = 2 / R2 = 1   # Random, but only between $Q_i(a)\\ where\\ i \\neq 1$\n",
    "- A3 = 2 / R3 = -2  # Greedy action, $Q_2(A_2) = 1$\n",
    "- A4 = 2 / R4 =2    # Random, $Q_3(A_2) = -0.5$ while $Q_3(A_3)\\ \\& \\ Q_3(A_4) = 0$\n",
    "- A5 = 3 / R5 =0    # Random / Greedy-Random betwen $A_3\\ \\& \\ A_4$\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**: In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.\n",
    "\n",
    "**Answer**: Since we are being asked on the long run, we will simplify and assume that the algorithm always chosses the optimal action when not in $\\epsilon$ (Exploratory) mode.  \n",
    "Now, let's look at the algorithms:  \n",
    "\n",
    "The difference between them is the $\\epsilon$, And we will look at an example of:  \n",
    "- $T: 1000$\n",
    "- $RL1: \\epsilon=0.1$ \n",
    "- $RL2: \\epsilon=0.3$\n",
    "- Notations:\n",
    "  - $\\mu(a^*)$: Mean of optimal action\n",
    "  - $\\mu(a)$: Mean for not-optimal actions\n",
    "\n",
    "So, what will be the difference in cummulative rewards between 1 and 2?\n",
    "\n",
    "$ T \\times [(1-\\epsilon_1)\\mu(a^*) + \\epsilon_1\\mu(a)] - [(1-\\epsilon_2)\\mu (a^*) + \\epsilon_2\\mu(a)] = $  \n",
    "$= T \\times [(\\epsilon_2 - \\epsilon_1)\\mu(a^*) + (\\epsilon_1 - \\epsilon_2)\\mu(a)] =$  \n",
    "$= T \\times [(\\epsilon_2 - \\epsilon_1)\\mu(a^*) - (\\epsilon_2 - \\epsilon_1)\\mu(a)] =$  \n",
    "$= T \\times [(\\epsilon_2 - \\epsilon_1)\\times(\\mu(a^*) - \\mu(a))] =$  \n",
    "$= T \\times \\Delta\\epsilon\\times\\Delta\\mu =$  \n",
    "$= 1000 \\times 0.2\\times(\\mu(a^*)-\\mu(a)) = 200 \\times \\Delta\\mu $\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1.5: Incremental Sample-Average  \n",
    "\n",
    "This is a computational imporvement over the last algortihm.  \n",
    "It uses a mathematical \"game\" to calculate the sample-average using 2 variables per arm only (instead of saving the whole reward history).  \n",
    "\n",
    "In the basic Sample-Average we use the value function:  \n",
    "$\\Large{Q_t(a) = \\frac{\\sum_{i=1}^{t-1}{R_i * \\mathbb{1}_{A_i=a}}}{\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i=a}}}}$  \n",
    "\n",
    "Which as we saw is very costly as we need to calculate $\\sum_{i=1}^{t-1}{R_i * \\mathbb{1}_{A_i=a}}$ at every timestamp.  \n",
    "Instead, we can calculate only ${NewEstimate}\\leftarrow{OldEstimate} + {StepSize}[{Target}-{OldEstimate}]$.  \n",
    "Or in our terms: $Q_{t+1} = Q_t + \\alpha[R_t - Q_t]$ where $\\alpha=\\frac{1}{n}$.  \n",
    "\n",
    "\n",
    "In this version all we need to keep at each timestamp is only $Q_t$ and use our $\\alpha$ and current $R_t$ to complete the update which takes *StepSize ($\\alpha$)* towards *Target ($R_t$)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Arm Bandit with Incremental-Sample-Average Q Function\n",
      "Total Reward: 29915.15448694125\n",
      "Optimal Reward: 38.88765981122818\tOptimal Total Reward: 77775.31962245636\n",
      "Regret: 47860.16513551511 / 61.54%\n",
      "arm 0:\t # chosen: 1846\t mean: 16.520862993020028\t true: 16.565519071273872\t delta: 0.0446560782538441\t delta %: 0.0027030112332939726\n",
      "\n",
      "arm 1:\t # chosen: 35\t mean: -0.667953330598793\t true: -8.995755230902391\t delta: 8.327801900303598\t delta %: 12.467640355708777\n",
      "\n",
      "arm 2:\t # chosen: 45\t mean: -2.4194504690103837\t true: -25.328656608932416\t delta: 22.909206139922034\t delta %: 9.468764264181228\n",
      "\n",
      "arm 3:\t # chosen: 39\t mean: 4.863880934113408\t true: 38.88765981122818\t delta: 34.02377887711477\t delta %: 6.995191563692472\n",
      "\n",
      "arm 4:\t # chosen: 35\t mean: -1.1294930182479717\t true: -18.46294490188693\t delta: 17.33345188363896\t delta %: 15.346223131618803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "class bandit_arm_v2():\n",
    "    def __init__(self, mean: float, std: float):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.distribution = np.random.normal\n",
    "        \n",
    "    def r(self):\n",
    "        while True:\n",
    "            yield self.distribution(loc=self.mean, \n",
    "                                    scale=self.std)\n",
    "\n",
    "class bandit_v2():\n",
    "    def __init__(self, k: int, eps: float):\n",
    "        self.A = self._create_arms(k)\n",
    "        self.R = dict()\n",
    "        self.epsilon = eps\n",
    "        self.n = 0\n",
    "        self.chosen = np.zeros(k)\n",
    "        self.total_reward = 0\n",
    "        self.cum_rewards = []\n",
    "        \n",
    "    def game(self, T: int):\n",
    "        self.R = self._init_rewards()\n",
    "        self.total_reward = 0\n",
    "        self.n = 0\n",
    "        for i in range(T):\n",
    "            self.n += 1\n",
    "            Rt = self.play()\n",
    "            self.total_reward += Rt\n",
    "            self.cum_rewards.append(self.total_reward)\n",
    "        return self.total_reward\n",
    "            \n",
    "    def play(self):\n",
    "        a = self._choose_action()\n",
    "        return self._do_action(a)\n",
    "        \n",
    "    def _create_arms(self, k: int):\n",
    "        return [bandit_arm_v2(np.random.normal(loc=0, scale=20), abs(np.random.normal(loc=0, scale=2))) for a in range(k)]\n",
    "    \n",
    "    def _init_rewards(self):\n",
    "        rewards = dict()\n",
    "        for arm in range(len(self.A)):\n",
    "            rewards[arm] = 0\n",
    "        return rewards\n",
    "    \n",
    "    def _choose_action(self):\n",
    "        r = np.random.uniform()\n",
    "        if r > self.epsilon:\n",
    "            Qt_with_indexes = [(self.R[k], k) for k in self.R.keys()]\n",
    "            Qt = [r[0] for r in Qt_with_indexes]\n",
    "            if Qt:\n",
    "                chosen_arm =  np.argmax(Qt)\n",
    "                chosen_arm = Qt_with_indexes[chosen_arm][1]\n",
    "            else:\n",
    "                chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        else:\n",
    "            chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        return chosen_arm\n",
    "    \n",
    "    def _do_action(self, a):\n",
    "        Rt = next(self.A[a].r())\n",
    "        alpha = (1/self.n)\n",
    "        self.R[a] += alpha * (Rt - self.R[a])\n",
    "        self.chosen[a] += 1\n",
    "        return Rt\n",
    "\n",
    "k = 5\n",
    "epsilon = 0.1\n",
    "b2 = bandit_v2(k, epsilon)\n",
    "T = 2000\n",
    "total_reward = b2.game(T)\n",
    "optimal_reward = np.max(list(map(lambda x: x.mean, b2.A)))\n",
    "total_optimal_reward = T * optimal_reward\n",
    "print('K-Arm Bandit with Incremental-Sample-Average Q Function')\n",
    "print(f'Total Reward: {total_reward}')\n",
    "print(f'Optimal Reward: {optimal_reward}\\tOptimal Total Reward: {total_optimal_reward}')\n",
    "print(f'Regret: {total_optimal_reward - total_reward} / {100*(1-(total_reward/total_optimal_reward)):.2f}%')\n",
    "for i in b2.R:\n",
    "    print(f'arm {i}:\\t # chosen: {b2.chosen[i]:0.0f}\\t mean: {b2.R[i]}\\t true: {b2.A[i].mean}\\t delta: {abs(b2.A[i].mean-b2.R[i])}\\t delta %: {abs(1-b2.A[i].mean/np.mean(b2.R[i]))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing Sample-Average bandit over 2000 turns\n",
      "110 ms ± 511 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f'Timing Sample-Average bandit over {T} turns')\n",
    "%timeit (b1.game(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing Incremental-Sample-Average bandit over 2000 turns\n",
      "29.4 ms ± 463 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f'Timing Incremental-Sample-Average bandit over {T} turns')\n",
    "%timeit (b2.game(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Stationary Methods  \n",
    "In the past algorithms we assumed that the Reward distribution in each of the arms stays the same throughout the process life.  \n",
    "However, in many cases and mostly in real life, the Reward is **Non-Stationary (Dynamic - Changes with time)**.  \n",
    "\n",
    "To accomodate for the Reward distribution change, we need to give our latest rewards higher value in our Q (value) function.   \n",
    "This will enable the algorithm to both remember its past rewards while taking into account the present situation more carefuly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimal change we can make to our current bandit algorithm to make it Non-Stationary is simply to use a **Fixed** $\\large\\alpha$\n",
    "\n",
    "So, If we use our latest example, the update rule $Q_{t+1}\\leftarrow Q_t + \\alpha[R_t - Q_t]$ with $\\alpha=\\frac{1}{n}$.  This time, we use a fixed $\\alpha\\in(0, 1]$.  \n",
    "\n",
    "The transition to a fixed $\\alpha$ leads to the following update equation:\n",
    "$Q_{n+1} = (1-\\alpha)^nQ_1 + \\sum_{i=1}^n{\\alpha(1-\\alpha)^{n-i}R_i}$.\n",
    "\n",
    "We call this equation a **Weighted Average** because:\n",
    "- The sum $(1-\\alpha)^n + \\sum_{i=1}^n{\\alpha(1-\\alpha)^{n-i}} = 1$\n",
    "\n",
    "We will also note that $\\alpha(1-\\alpha)^{n-i}$ on $R_i$ increases as we get closer to $n$, **giving rescent rewards higher weight then earlier ones.**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.4**: If the step-size parameters, $\\alpha_n$, are not constant, then the estimate $Q_n$ is a weighted average of previously received rewards with a weighting different from that given by (2.6) - ($Q_{t+1}\\leftarrow Q_t + \\alpha[R_t - Q_t]$ with $\\alpha=\\frac{1}{n}$)\n",
    "- What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "In the dynamic $\\alpha$ case wheren $\\alpha$ is always versioned as $\\alpha_i$ the analysis is as follows:  \n",
    "\n",
    "$Q_{n+1} \\leftarrow Q_n + \\alpha_n(R_n - Q_n) = $  \n",
    "$Q_{n+1} \\leftarrow \\alpha_{n}R_n + (1-\\alpha_n)Q_n = $  \n",
    "$Q_{n+1} \\leftarrow \\alpha_{n}R_n + (1-\\alpha_n)[\\alpha_{n-1}R_{n-1} + (1-\\alpha_{n-1})Q_{n-1}] =  ...  $  \n",
    "$Q_{n+1} \\leftarrow \\alpha_{n}R_n + \\sum_{i=n-1}^{1}{[\\alpha_{i}R_i\\times\\Pi_{j=n}^{i}{(1-\\alpha_j)}]} + Q_1 $  \n",
    "\n",
    "So, by that we can see the reward's priors are $\\alpha_n\\times\\Pi_{j=n}^{i}{(1-\\alpha_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.5 (programming)**: Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the $q_*(a)$ start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the $q_*(a)$ on each step). Prepare plots like Figure 2.2 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, $\\alpha$ = 0.1. Use $\\epsilon$ = 0.1 and longer runs, say of 10,000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Constant Alpha**\n",
      "K-Arm Bandit with Incremental-Sample-Average Q Function\n",
      "Total Reward: 321242.4998739323\n",
      "Optimal Reward: 55.10699100579213\tOptimal Total Reward: 551069.9100579213\n",
      "Regret: 229827.41018398904 / 41.71%\n",
      "arm 0:\t # chosen: 224\t mean: -7.50438171240501\t true: -0.9590284442988851\t delta: 6.545353268106125\t delta %: 0.872204202684203\n",
      "\n",
      "arm 1:\t # chosen: 201\t mean: -26.26377949774405\t true: -30.913433595080818\t delta: 4.649654097336768\t delta %: 0.1770367474238106\n",
      "\n",
      "arm 2:\t # chosen: 2965\t mean: -10.310591591009116\t true: -12.111606208518465\t delta: 1.8010146175093489\t delta %: 0.17467616689228982\n",
      "\n",
      "arm 3:\t # chosen: 862\t mean: -20.526859890736702\t true: -23.28601262162964\t delta: 2.7591527308929393\t delta %: 0.13441669819834856\n",
      "\n",
      "arm 4:\t # chosen: 5748\t mean: 56.51063161623431\t true: 55.10699100579213\t delta: 1.4036406104421815\t delta %: 0.024838522775224914\n",
      "\n",
      "**Dynamic Alpha**\n",
      "K-Arm Bandit with Incremental-Sample-Average Q Function\n",
      "Total Reward: 543787.1070933247\n",
      "Optimal Reward: 29.78689548911103\tOptimal Total Reward: 297868.9548911103\n",
      "Regret: -245918.15220221435 / -82.56%\n",
      "arm 0:\t # chosen: 211\t mean: -0.4178811534310543\t true: -19.711672233378263\t delta: 19.29379107994721\t delta %: 46.170522220333794\n",
      "\n",
      "arm 1:\t # chosen: 200\t mean: 0.1347377211397828\t true: 11.116453355516516\t delta: 10.981715634376734\t delta %: 81.50438898238319\n",
      "\n",
      "arm 2:\t # chosen: 243\t mean: 0.08202686181348966\t true: -10.193224154744692\t delta: 10.275251016558181\t delta %: 125.26690390669529\n",
      "\n",
      "arm 3:\t # chosen: 192\t mean: -0.6778240393047622\t true: -13.338494019001882\t delta: 12.66066997969712\t delta %: 18.678402130268278\n",
      "\n",
      "arm 4:\t # chosen: 9154\t mean: 59.27499893190562\t true: 29.78689548911103\t delta: 29.488103442794593\t delta %: 0.4974796115419615\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "class bandit_arm_v3():\n",
    "    def __init__(self, mean: float, std: float):\n",
    "        # Start equally\n",
    "        self.mean = 0\n",
    "        self.std = 0.1\n",
    "        \n",
    "        # Random walk parameters\n",
    "        self.mean_change = mean\n",
    "        self.std_change = std\n",
    "        \n",
    "        self.distribution = np.random.normal\n",
    "        \n",
    "    def random_walk(self):\n",
    "        self.mean += self.distribution(loc=self.mean_change,\n",
    "                                       scale=self.std_change)\n",
    "        \n",
    "    def r(self):\n",
    "        while True:\n",
    "            self.random_walk()\n",
    "            yield self.distribution(loc=self.mean, \n",
    "                                    scale=self.std)\n",
    "\n",
    "class bandit_v3():\n",
    "    def __init__(self, k: int, eps: float, const_alpha: bool):\n",
    "        self.A = self._create_arms(k)\n",
    "        self.R = dict()\n",
    "        self.epsilon = eps\n",
    "        self.n = 0\n",
    "        self.chosen = np.zeros(k)\n",
    "        self.total_reward = 0\n",
    "        self.cum_rewards = []\n",
    "        self.is_optimal = 0\n",
    "        self.cum_optimal = []\n",
    "        self.const_alpha = const_alpha\n",
    "        \n",
    "    def game(self, T: int):\n",
    "        self.R = self._init_rewards()\n",
    "        self.total_reward = 0\n",
    "        self.n = 0\n",
    "        for i in range(T):\n",
    "            self.n += 1\n",
    "            Rt = self.play()\n",
    "            self.total_reward += Rt\n",
    "            self.cum_rewards.append(self.total_reward)\n",
    "            self.cum_optimal.append(self.is_optimal / (i+1))\n",
    "        return self.total_reward\n",
    "            \n",
    "    def play(self):\n",
    "        a = self._choose_action()\n",
    "        self.is_optimal += self._is_optimal(a)\n",
    "        return self._do_action(a)\n",
    "        \n",
    "    def _create_arms(self, k: int):\n",
    "        return [bandit_arm_v3(0, 1) for a in range(k)]\n",
    "    \n",
    "    def _get_optimal_hand(self):\n",
    "        hands_rewards = list(map(lambda arm: arm.mean, self.A))\n",
    "        return np.argmax(hands_rewards)\n",
    "    \n",
    "    def _is_optimal(self, chosen_hand: int):\n",
    "        return 1 if chosen_hand == self._get_optimal_hand() else 0\n",
    "    \n",
    "    def _init_rewards(self):\n",
    "        rewards = dict()\n",
    "        for arm in range(len(self.A)):\n",
    "            rewards[arm] = 0\n",
    "        return rewards\n",
    "    \n",
    "    def _choose_action(self):\n",
    "        r = np.random.uniform()\n",
    "        if r > self.epsilon:\n",
    "            Qt_with_indexes = [(self.R[k], k) for k in self.R.keys()]\n",
    "            Qt = [r[0] for r in Qt_with_indexes]\n",
    "            if Qt:\n",
    "                chosen_arm =  np.argmax(Qt)\n",
    "                chosen_arm = Qt_with_indexes[chosen_arm][1]\n",
    "            else:\n",
    "                chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        else:\n",
    "            chosen_arm = np.random.choice(range(len(self.A)))\n",
    "        return chosen_arm\n",
    "    \n",
    "    def _do_action(self, a):\n",
    "        Rt = next(self.A[a].r())\n",
    "        alpha = 0.1 if self.const_alpha else (1/self.n)\n",
    "        self.R[a] += alpha * (Rt - self.R[a])\n",
    "        self.chosen[a] += 1\n",
    "        return Rt\n",
    "\n",
    "k = 5\n",
    "epsilon = 0.1\n",
    "T = 10000\n",
    "bandit_const_alpha = bandit_v3(k, epsilon, True)\n",
    "bandit_dynamic_alpha = bandit_v3(k, epsilon, False)\n",
    "\n",
    "def run_simulation(bandit: bandit_v3):\n",
    "    total_reward = bandit.game(T)\n",
    "    optimal_reward = np.max(list(map(lambda x: x.mean, bandit.A)))\n",
    "    total_optimal_reward = T * optimal_reward\n",
    "    print('K-Arm Bandit with Incremental-Sample-Average Q Function')\n",
    "    print(f'Total Reward: {total_reward}')\n",
    "    print(f'Optimal Reward: {optimal_reward}\\tOptimal Total Reward: {total_optimal_reward}')\n",
    "    print(f'Regret: {total_optimal_reward - total_reward} / {100*(1-(total_reward/total_optimal_reward)):.2f}%')\n",
    "    for i in bandit.R:\n",
    "        print(f'arm {i}:\\t # chosen: {bandit.chosen[i]:0.0f}\\t mean: {bandit.R[i]}\\t true: {bandit.A[i].mean}\\t delta: {abs(bandit.A[i].mean-bandit.R[i])}\\t delta %: {abs(1-bandit.A[i].mean/np.mean(bandit.R[i]))}\\n')\n",
    "    return bandit\n",
    "\n",
    "print('**Constant Alpha**')\n",
    "bc = run_simulation(bandit_const_alpha)\n",
    "print('**Dynamic Alpha**')\n",
    "bd = run_simulation(bandit_dynamic_alpha)\n",
    "# sns.lineplot(range(T), b2.cum_optimal)\n",
    "\n",
    "\n",
    "plt.plot(bc.cum_optimal, label='const_alpha (0.1)')\n",
    "plt.plot(bd.cum_optimal, label='dynamic_alpha (1/n)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimistic Initial Values\n",
    "In **Stationary Problems** ($\\alpha$ is constant) we can use a nice trick to save us exploration time.  \n",
    "If we set the *Initial Values* to be Higher then what we expect, the sample-average algorithm will start by doing the exploration on the different actions and converge faster (or with less $\\epsilon$) to the right action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.6**: Mysterious Spikes The results shown in Figure 2.3 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?\n",
    "\n",
    "**Answer**: In the *Optimistic* algorithm we **Over-Value** the actions (being optimistic) in our initial evaluation.  \n",
    "Because of this optimism, when the algorithm chooses the next **greedy** action, it chooses one of the over-evaluated hands.  When this happens our optimism meets reality and the value estimation $q(a)$ drops to the action's real value.  \n",
    "When the value begins to drop, the other hands that are still over-valued are being chosen instead but then again we meet the usually harsher truth and the $q(a)$ drops.  This process keeps happening until we reach truthful evaluations for the possible actions.\n",
    "\n",
    "So, The optimistic algorithm has 2 exploration mechanisms.\n",
    "- $\\epsilon$-greedy: Constant exploration rate\n",
    "- Optimistic initial values: Early phase forced exploration\n",
    "\n",
    "While the normal algorithm only has the $\\epsilon$-greedy constant exploration rate and Naive initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB - Upper-Confidence-Bound Action Selection\n",
    "Exploration is always needed in a MAB environment.   However, we can try and explore in a better way then simply randomly selecting an action.  \n",
    "UCB is a method to select better exploration paths by using our gathered knowledge on the actions from our previous selections.\n",
    "\n",
    "UCB is based on calculating the Upper Confidence Bound for the action's reward distribution which is given by:  \n",
    "$A_t = \\underset{a}{\\arg\\max}[{Q_t(a) + c\\sqrt{\\frac{\\ln(t)}{N_t(a)}}}]$\n",
    "* $N_t(a)$: Number of times $a$ has been selected prior to time $t$\n",
    "* $c$: exploration confidence\n",
    "\n",
    "The term under the sqrt estimates the uncertainty about $Q_t(a)$.  \n",
    "When $a$ is selected, $N_t(a)$ increases. When $a$ is not selected, $\\ln(t)$ increases.  \n",
    "We can also see that although $\\ln(t)$ is unbounded, it is increasing by a smaller amount each time giving $N_t(a)$ \"an edge\"\n",
    "\n",
    "\n",
    "Knowing this uppper confidence bound enables us to estimate early on if an action can be the optimal one.  \n",
    "We can now compare the UCB for each a instead of the $Q_t(A)$ directly.\n",
    "We can use the same to calculate the LCB (Lower-Confidence-Bound).  When the $UCB_{a1} <= LCB_{a2}$ for some ($a1 \\ne a2) \\in A$ we can safely stop exploring or choosing this action.\n",
    "\n",
    "\n",
    "Although good for stationary problems, the UCB algorithm doesn't work for non-stationary problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**UCB**\n",
      "K-Arm Bandit with UCB Q Function\n",
      "Total Reward: 7817.556411409742\n",
      "Optimal Reward: 23.25300982224664\tOptimal Total Reward: 23253.00982224664\n",
      "Regret: 15435.453410836897 / 66.38%\n",
      "arm 0:\t # chosen: 165\t mean: -394.3607586859547\t true: -7.2682831237619885\t delta: 387.0924755621927\t delta %: 0.9815694564845129\n",
      "\n",
      "arm 1:\t # chosen: 246\t mean: -653.0734867482004\t true: -9.887362271508097\t delta: 643.1861244766923\t delta %: 0.9848602607943258\n",
      "\n",
      "arm 2:\t # chosen: 60\t mean: -189.71659164385824\t true: -2.05270666800647\t delta: 187.66388497585177\t delta %: 0.9891801415457648\n",
      "\n",
      "arm 3:\t # chosen: 514\t mean: 9115.163527497547\t true: 23.25300982224664\t delta: 9091.9105176753\t delta %: 0.9974489750236406\n",
      "\n",
      "arm 4:\t # chosen: 15\t mean: -60.456279009790414\t true: -5.402430853590285\t delta: 55.05384815620013\t delta %: 0.9106390445777286\n",
      "\n",
      "K-Arm Bandit with UCB Q Function\n",
      "Total Reward: -9465.301191014058\n",
      "Optimal Reward: -12.497042415711423\tOptimal Total Reward: -12497.042415711423\n",
      "Regret: -3031.7412246973654 / 24.26%\n",
      "arm 0:\t # chosen: 183\t mean: -3.035264731918004\t true: -14.33136515883269\t delta: 11.296100426914684\t delta %: 3.721619504265318\n",
      "\n",
      "arm 1:\t # chosen: 202\t mean: -3.03721700342515\t true: -17.353851976100493\t delta: 14.316634972675343\t delta %: 4.7137346315821675\n",
      "\n",
      "arm 2:\t # chosen: 233\t mean: -3.036856274735653\t true: -12.497042415711423\t delta: 9.46018614097577\t delta %: 3.1151247491286833\n",
      "\n",
      "arm 3:\t # chosen: 236\t mean: -3.0423286744280085\t true: -16.163373789400882\t delta: 13.121045114972874\t delta %: 4.312829585199165\n",
      "\n",
      "arm 4:\t # chosen: 146\t mean: -3.0473488137487887\t true: -19.14857465839685\t delta: 16.101225844648063\t delta %: 5.2836832370497975\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/orz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/orz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xd4U9f5wPHvsS0P8AI8ANvggdkbswkQQhJCAmQ2obQNzWyaZrTNbtqM/ppmNCVNs0qz05SQQRJCaVLCCIGwDJi9bDB4MLyxjbfO748jDzywMLJlye/nefRIujq6OteCV+e+59xzlNYaIYQQ7sXD2RUQQgjheBLchRDCDUlwF0IINyTBXQgh3JAEdyGEcEMS3IUQwg1JcBdCCDckwV0IIdyQBHchhHBDXs764JCQEB0dHe2sjxdCCJe0devWbK11aHPlnBbco6OjSUxMdNbHCyGES1JKHbWnnKRlhBDCDUlwF0IINyTBXQgh3JDTcu5CiPavoqKC9PR0SktLnV2VDsfX15fIyEgsFkuL3i/BXQjRpPT0dAICAoiOjkYp5ezqdBhaa3JyckhPTycmJqZF+2g2LaOUelspdUoptbuJ15VS6mWlVLJSaqdSamSLaiKEaHdKS0vp1q2bBPY2ppSiW7duF3TGZE/O/V1gxjlevwKIt93uAF5vcW2EEO2OBHbnuNC/e7NpGa31WqVU9DmKzAHe12a9vo1KqWClVA+t9fELqllTjm6AlFW1z2MmQ8xFrfJRQgjhqhwxWiYCSKvzPN22rQGl1B1KqUSlVGJWVlbLPi19M6x9wXZ7Hr59smX7EUK4hNTUVAYPHnzWtieffJK//OUvAPzlL3+hf//+DB48mGHDhvH+++8DMHXqVPr168fw4cMZMGAACxcubPO6O5MjOlQbO3dodNVtrfVCYCFAQkJCy1bmnnifuQH8+yY4ndGi3QghXN8bb7zBihUr2Lx5M4GBgRQUFPDFF1/UvP7hhx+SkJBAbm4ucXFxzJ8/H29vbyfWuO04IrinA1F1nkcCmQ7Yb/M8PMFa2SYfJYRof5555hlWr15NYGAgAEFBQdx8880NyhUVFdG5c2c8PT3buopO44jgvhT4lVLqI2AsUNBq+fb6PLwkuAvRRp76ag97M087dJ8DewbyxKxBLXpvSUkJhYWFxMXFNVlm3rx5+Pj4cOjQIV566SUJ7nUppRYBU4EQpVQ68ARgAdBavwEsB2YCycAZ4OetVdkGPC0S3IVwc02NGrFarc2OKKlOy2RlZTFhwgRmzJhB7969W6Oa7Y49o2XmNvO6Bu52WI3Oh4cXVElwF6IttLSFfaG6detGXl7eWdtyc3MZNWoUnTt35vDhw8TGxp5zH6GhoYwcOZJNmzZ1mODu2nPLSM5dCLfn7+9Pjx49WLlyJWAC+9dff82kSZN49NFHufvuuzl92qSLTp8+3eiomDNnzrB9+/ZzpnDcjWtPP+AhaRkhOoL333+fu+++m9/+9rcAPPHEE8TFxXHXXXdRVFTE6NGjsVgsWCyWmjJgcu5+fn6UlZUxf/58Ro0a5axDaHMuHty9wFrh7FoIIVrZwIEDWb16dYPtSikeeughHnrooQavrVmzpg1q1n65eFrGC6xVzq6FEEK0O64d3D1lKKQQQjTGtYO7hxdUSVpGCCHqc/3gLi13IYRowMWDuwXQYLU6uyZCCNGuuHhwt11KXHTCufUQQoh2xsWDu20k58sjnFsPIUSrOXHiBDfddBNxcXEMHDiQmTNncvDgQYft/4svvmDv3r3nLDNs2DDmzj37Yv358+fz6aefnvN99pRpLa4d3CvOmPtKWbxXCHekteaaa65h6tSppKSksHfvXp555hlOnjzpsM9oLrjv27cPq9XK2rVrKS4udtjntjbXDu7Hdzi7BkKIVrR69WosFgu/+MUvarYNHz6ciy66CK01Dz74IIMHD2bIkCEsXrwYMBcvTZ06leuvv57+/fszb948zBRY8MgjjzBw4ECGDh3KAw88wA8//MDSpUt58MEHGT58OCkpKQ3q8O9//5uf/vSnXHbZZSxdurTRekZHR/Pwww8zZswYxowZQ3Jycs1ra9euZcKECcTGxta04ouKirjkkksYOXIkQ4YM4csvv3TY36yaa1+hOvZOOPg1hPR1dk2EcH//fQRO7HLsPrsPgSuebfLl3bt3NzllwJIlS0hKSmLHjh1kZ2czevRoJk+eDMD27dvZs2cPPXv2ZOLEiaxfv56BAwfy+eefs3//fpRS5OfnExwczOzZs7nqqqu4/vrrG/2cxYsXs2LFCg4cOMArr7zSID1TLTAwkM2bN/P+++9z//33s2zZMgCOHz/OunXr2L9/P7Nnz+b666/H19eXzz//nMDAQLKzsxk3bhyzZ8926Hq1rt1yj5sG8ZeDxc/ZNRFCtLF169Yxd+5cPD09CQ8PZ8qUKWzZsgWAMWPGEBkZiYeHB8OHDyc1NZXAwEB8fX257bbbWLJkCZ06dWr2M7Zs2UJoaCi9e/fmkksuYdu2bQ1mqKxWHfTnzp3Lhg0barZfffXVeHh4MHDgwJp0ktaaxx57jKFDhzJ9+nQyMjIcmmoCV2+5A3h5y4VMQrSFc7SwW8ugQYOa7JCsTrU0xsfHp+axp6cnlZWVeHl5sXnzZlauXMlHH33EK6+8wqpVq875+YsWLWL//v1ER0cDZtbJzz77jNtuu61B2bqt7rqP69alus4ffvghWVlZbN26FYvFQnR0NKWlju07dO2WO4CnN1SWObsWQohWMG3aNMrKyvjnP/9Zs23Lli189913TJ48mcWLF1NVVUVWVhZr165lzJgxTe6rqKiIgoICZs6cyUsvvURSUhIAAQEBFBYWNihvtVr55JNP2LlzJ6mpqaSmpvLll1+yaNGiRvdfnfNfvHgx48ePP+dxFRQUEBYWhsViYfXq1Rw9erTZv8X5cv2Wu6e03IVwV0opPv/8c+6//36effZZfH19iY6O5qWXXmLy5Mls2LCBYcOGoZTi+eefp3v37uzfv7/RfRUWFjJnzhxKS0vRWrNgwQIAbrrpJm6//XZefvllPv3005o539euXUtERAQRERE1+5g8eTJ79+7l+PGGK4mWlZUxduxYrFZrkz8A1ebNm8esWbNISEhg+PDh9O/fv6V/oiapc53atKaEhASdmJh44Ttaei8c/AYeOHDh+xJCnGXfvn0MGDDA2dVo96Kjo0lMTCQkJMSh+23s76+U2qq1Tmjuve6RlqmStIwQQtTl+mkZLx9JywghnCo1NdXZVWjADVruFulQFaIVOSt129Fd6N/dDYK7t1lqT/4BCuFwvr6+5OTkSIBvY1prcnJy8PX1bfE+XD8t4+lt7qvKTYpGCOEwkZGRpKenk5WV5eyqdDi+vr5ERka2+P0S3IUQTbJYLMTExDi7GqIFXD8tUx3QpVNVCCFquH5w97SYe+lUFUKIGm4Q3OukZYQQQgBuEdyr0zIS3IUQopobBHdbWkaCuxBC1HD94O4lLXchhKjPruCulJqhlDqglEpWSj3SyOu9lFKrlVLblVI7lVIzHV/VJtR0qEpwF0KIas0Gd6WUJ/AqcAUwEJirlBpYr9jjwMda6xHATcBrjq5okyTnLoQQDdjTch8DJGutD2uty4GPgDn1ymgg0PY4CMh0XBWbUTNaRoZCCiFENXuuUI0A0uo8TwfG1ivzJPA/pdQ9QGdgukNqZw9JywghRAP2tNwbW467/ixCc4F3tdaRwEzgA6VUg30rpe5QSiUqpRIdNleFl21inUrHrj8ohBCuzJ7gng5E1XkeScO0y63AxwBa6w2AL9BgSRKt9UKtdYLWOiE0NLRlNa7PUh3cJS0jhBDV7AnuW4B4pVSMUsob02G6tF6ZY8AlAEqpAZjg3jbTyHn5mfvKkjb5OCGEcAXNBnetdSXwK+AbYB9mVMwepdTTSqnZtmK/BW5XSu0AFgHzdVtNAF3dcq+QtIwQQlSza8pfrfVyYHm9bX+o83gvMNGxVbOTtNyFEKIBN7lCVUnLXQgh6nD94K6UGTEjLXchhKjh+sEdTOtdWu5CCFHDPYK7xU9a7kIIUYd7BHcvXxnnLoQQdbhHcLf4QYW03IUQopp7BHcvX5l+QAgh6nCP4G7xkw5VIYSowz2CuwyFFEKIs7hHcHfzlntpRRXllVZnV0MI4ULcI7h7+bhty33f8dNc9Pxq7l+83dlVEUK4EDcJ7u7Zct9+LI8b/7GB3OJylu86QfKpImdXSQjhItwjuFvcL+eelJbPz97aTHAnb5bcNQFvLw/eWnfE2dUSQpyniiorezNP83FiGn/4cjfXvraeFXtPtvrn2jUrZLvnZhcx7UjL56dvbaJLZ28+umMcPYP9uG5kBEu2pfPAZX3p5u/j7CoKIRpRVlnFoZNF7MooYLfttu9EYU2fWWdvTwb1DMKjsfXtHMw9gnv1RUxam4nEXNjujAJ++tYmgjtZWGQL7AC3Toph0eY0/rXxGPdNj3dyLYUQFVVWDpwoZEd6PrvSC9idWcCBE4VUVJmlLAJ8vBgUEcjN43szOCKIwRFBxHTrjEdbRHbcJbh7+QIaqsptUwC7pmM5Z5j/zhYCfC0sun0cEbbADtAnLICL+4XywcZU7pwSi6/F04k1FaJj0VqTkV9CUlo+ScfySUrLZ3dmAaUVpkUe5GdhSEQQt0yKYUhEEIN7BtGra6c2C+SNcY/gbrEFwYoSlw3uOUVl3PzOZiqqrHx0x1giu3RqUOa2i2KZ9+YmPt+ewdwxvZxQSyE6htOlFexMKyApLY+ktAKS0vLJLjKpX28vDwb1DGTumF4MjwpmRFQXorr6odpZ1sA9grtX9SLZrjlipqS8ilveSyQzv4R/3z6WPmEBjZabENeNoZFBvL4mhRtGReLl6R794UI4k9aaY7ln2JKaR2JqLluP5pGcVUT1QqGxoZ2ZHB/C8F7BDI8Kpn/3QLy92v//PfcK7i44eZjWmoc/28nO9Hz+8ZNRjOrdtcmySinumRbP7e8n8mVSJteNimzDmgrhHqpHr2yxBfItqXk1rfJAXy9G9e7CrGE9GR4VzLDIYII6WZxc45Zxj+Bucd2W+8K1h1m6I5MHL+/HZYO6N1t++oAw+ncP4NU1yVw9IgJPJ+b0hHAFRWWVbD2ax9bUXLak5pGUlk9JRRUAUV39uCg+hIToLiT07kp8mL9T8+SO5B7B3atOzt2FfHcwi+e+3s+VQ3vwy6lxdr2nuvV+97+3sXzXcWYN69nKtRTCtRSXVZJ4NI8NKTlsOJzD7owCqqwaDwUDewZy4+iommDePcjX2dVtNe4R3L1tnY8uFNwz80u4d9F2+oYH8ML1Q8+rM+aKwd3pE+bPK6uSuXJID7dpaQjREiXlVWw9msfGwyaY70jLp9Kq8fJQDI8K5q4pcYyN7cqIXl3w93GPkGcP9zhSb39zX17s3HrYqcqquf+jJCqrrLz+k1F08j6/r8HDQ3HPtD7c91ES/5HWu+hgyiutbD+Wx/qUHDam5JCUlk95lRVPD8WQiCBunxzL+NhuJER3Oe//W+7EPY7cu7O5Ly90bj3s9MqqZDan5vLXHw0jJqRzi/Zx1dCevL4mhb+uOMiMwd2xyMgZ4aa01hzNOcPaQ1msPZjNhpRsisur8FAwqGcQ8ydG1wTzAF/X7PxsDW4S3F2n5b4lNZe/rTzItSMiuHZky0e7eHooHry8H7e+l8jHiWnMG9vbgbUUwrlOl1bwQ3IO3x/KYu2hLNJyTco1sosfc0ZEMDk+lPFx3Qjyk2DeFDcJ7tUt9/Yd3EvKq/jtxzuI7NKJp68efMH7m9Y/jITeXfjbt4e4dkQkft5y1apwTVpr9mSeZtX+U6w9mMX2tHyqrJrO3p6Mjwvh9otimRwfSu9undrdxULtlZsEd1vLvax9T4n74v8OcCz3DB/dMc4hHTtKKR6+oj83vLGBd39I5S47R9wI0R6UlFfxQ0o23+47xer9pzhxuhSlYEhEEHdNieOi+BBG9u4iKccWco/g7uUNnt5Q3j6De05RGX9ctpcvkjKZN7YX42K7OWzfo6O7Mq1/GK+vSWbumCiCO3k7bN9CONrxghJW7T/Fqn2nWJecTVmllc7enkzuG8q0/mFc3D+MEJn11CHcI7iDSc200+B+y3uJ7EjLB+CRK/o7duda89DlfZn58jpe+vYQT84e5Nj9C3EBtNbsP1HI17tP8O2+k+zJPA2Y3PncMb24ZEAYY2K64uMlKUVHc6PgHtAuc+4/pGTXBPaP7hjn2N78kjx4/2r6hw/ix2Pv4oONR/nx2F70DW98bhoh2oLWmh3pBXy9+wRf7z5Oas4ZlIJRvbrw8Iz+TB8QRp8wf8mdtzI3Cu6doax9DYWsrLLy1NK9RHbx49vfTHHsNL2VZfDRT+B4EhxP4pHZ0/gqKYCnv9rLB7eOkf84ok1VWTWJqbl8vecE3+w+QWZBKV4eivFx3bhjchyXDgwnNEDSLW3JruCulJoB/A3wBN7UWj/bSJkfAU8CGtihtf6xA+vZPB//dtdyX7T5GAdOFvL6vJGOC+zpibBoLhSfMs9n/x2W3oP/0lv53i+KYcl/5n97T3K5HfPUCHEhrFbN5tRcvtqRyTd7TpBdVI63lweT40P5zWX9mD4gTPqAnKjZ4K6U8gReBS4F0oEtSqmlWuu9dcrEA48CE7XWeUqpsNaqcJO8O7er4F5wpoIXVxxkfGw3Zgx2UKDNOwpvXlL7fNrjMPJn0DUW3r2SwJI07uvyA//3n85M6RsqC3oIh6sesvhlUgbLdh7neEEpfhZPpvUPY8bg7lzcP6xDXeLfntnzLYwBkrXWhwGUUh8Bc4C9dcrcDryqtc4D0FqfcnRFm+XtD0Vt/7FN+ef3h8k/U8HjVw248BSJ1pCTAp/MB59AuOz/IHYKdIk2r0dPgify4b1Z3J35b947PZTX1qTwm0v7XuhhCAFASlYRS5My+WpHJoezi7F4Kqb0DeWRK/pz6cDwDn2Zf3tlzzcSAaTVeZ4OjK1Xpi+AUmo9JnXzpNb66/o7UkrdAdwB0KuXg1cS8vZvN6NlsovKeHv9Ea4c2oNBPYMubGeZSbBwSu3zH38CfS9rWE4pmPkCljcm8Vr4V/xsTRCzh/VocuEPIZqTU1TGF0mZfL49nd0Zp1EKxsV04/bJsVwxuLukXNo5e4J7Y81O3ch+4oGpQCTwvVJqsNY6/6w3ab0QWAiQkJBQfx8XxruzYy5iKj0Nb10Gl/8J+lzSfPlGvLEmhdKKKn49/QJbzmVFprVe7coXGw/s1cIGwLi7GP/D3xnnPYFHlwSz+I7xMmuksFtFlZXV+0/x6dZ0Vu0/RaVVMyQiiMevHMBVQ3u69RS57sae4J4ORNV5HglkNlJmo9a6AjiilDqACfZbHFJLeziiQ9VqhVV/hKx9sG9pi4L7iYJS3t94lGtHRtInzP/C6vPfhyH/KMxfDtET7XvPlIdh16e8WfoClx39Ax9tieTHY2W9VXFu+46f5tOt6XyxPYOc4nJC/H24ZVIM142MpF93OftzRfYE9y1AvFIqBsgAbgLqj4T5ApgLvKuUCsGkaQ47sqLN8vaHqjKoqgDPFo4l3/B32LzQPE7b3KJdvL4mGatVc98l8S2rQ7U9n0PSv+CiB+wP7AA+AXDFc/h8/DO+8/kNF/33TaYPCCMsUFpc4mynSyv4YnsGHyemsTvjNBZPxfQB4Vw/KpIpfUNljV4X12xw11pXKqV+BXyDyae/rbXeo5R6GkjUWi+1vXaZUmovUAU8qLXOac2KN1AzM2QR+HVp2T4Of1f7+NQ+KMkHv2C7355bXM7ixDSuGRFBVNdOLavDjsXw+R3mcUQCTH3k/PcxYDZMfhDWvsAj1rd4dEk0r/1kpFwFKADYlV7Ah5uO8mVSJiUVVQzsEciTswYye3gEXTtLHt1d2NXFrbVeDiyvt+0PdR5r4De2m3NUzwxZ1sLgrjWcsg0AmvEcfP0wZCRCn+nNvnXpjkwKzpSTXVROaYWVO6fEnv/nA5zcWxvYAa77Z8vOQpQywyS9fLhy1f+x7OBX9Hv8FEMjg3jv52PoIv+BO5wz5ZV8tSOTDzcdY2d6AX4WT2YP68m8cb0YGml/A0a4DvcZv+Rjywu29CrV7INQeBxm/Q0GXwffPAppW5oN7icKSrl30faa59MHhLdshIrVCsvuN2cgc16FfjPNhGgXYuKv0fuW8dzJd9l8pj870+Gxz3fx2ryRcgVrB5F8qpAPNhxlybYMCssq6Rvuz1OzB3H1iAiZC93NuU9wr06flOafu1xTUlab+9iLzQ9F+CBI29Ts2xZvSTvr+S9a2mrf+o75vKvfgEFXt2wf9Xl6oa55g8B/TOb7Qct4J+IpXvjfQT5OTOPG0dLJ6q6sVs13h7J4Z30qaw9m4e3pwcwh3Zk3rjcJvbvID3sH4T7B3bc6uBe07P2H10CXGOhiW9EoaqzJf1urwKPxXHVllZVFm48xuW8og3oGcjiriIToruf3uVrDd8/DmmcgZgoMu6ll9W9K2ACY+iidVj7FXYOv5Ic+cTz82S62Hc3n2euGyH90N3KmvJLPtmXwzvojHM4qJizAhwcu68vcMb3oJtPodjhuFNxtFwuVtKDlbrXC0R9g0JzabVFjYcubJg/ffUijb1tpW2DgqTmDWj6Xy94vTGAHuGqByZc72sT7IHklHssf5KV5KxidnMPixDQGRwbx03GyPJ+ry8wv4b0NqSzadIzTpZUMiQjipRuHM3NID7y9ZMRLR+U+wb26E7UlLfesfVBWAL0m1G7rNd7cp65rMrj/a+NRugf6ckn/Fk6lU1EC//u9eTx3MXRrpZWUPDzh2oXwxkRCv/4Fh59ewS0f7uSPX+1laEQQw6Jcp0Mtt7iclKwiRp/vGZIbOnSykDe+O8yXSRlYtWbG4O7cMjGGUZJ6EYD7/Kz7BJr7luTcj200973qzKoQHGUm5Ko7PLKO9LwzfH8omxtHR7VsPPCp/fCn7lCQBvP/A/1mnP8+zkdQBMx5DU7sxGPlkyz40XBCA3z45YfbyCsux2p17AXDrWHNgVNctmAtN7yxgSeX7qGiyursKjnF9mN53PF+IpcuWMvyXcf56fjefPfgxbw2bxQJ0V0lsAvAnVrunl5mwY6WtNyPbQT/cJNzrytmCuz+DKoqzf7r+DLJXKR7/ajIltV3hW0kaezFZuKvttB/Joy5Eza9TpeYi3ht3kRueGMDo//0LX4WTx6dOaBdXs1aXmnl2f/u5+31R+gXHsClA8N594dUDpwo5NV5IzvE2GytNeuSs3ltdQobDucQ5Gfh3kvimT8hukMcvzh/7tNyB5N3b0nOPW0j9BrXMN8dMxnKTpsFMerQWvP59gxGR3dp2cVKxzbBoW/got/CvE/P//0X4tKnoccw+PwXDPPL5vezBlJp1RSWVfLY57u49rX1lFZUtW2dziE97ww3/GMDb68/ws3je/Plryby52uH8OINw9h6LI9Zf1/HnswCyiqr2Hf8tLOr63Baa1buO8mcV9fz07c2czi7iN/NHMD6R6bxm0v7SmAXTXKfljuY4ZDnm5Y5nQn5x2DsXQ1fi5ls7g+vgciEms17Mk+TfKqIP10z+Pw+q+gUfPMYZGyFzmEmuHu28Vdg8YUb/wX/mAKL5/GTW1cQEZxAn9AA5r21kW3H8nno050suHE4nk6ecGzV/pP8evEOrFbN6/NGcsWQHjWvXTfKzN1z5wdbue71H4gI9iMlq5i5Y3rxxKyBLj+XvdaaNQezeGnFQXakFxDV1Y9nrx3CNSMj5EpjYRf3a7mfb1qmeix7r/qzGAOdQyB8MBxZe9bmJdsy8Pb04KohPc2GlU/DvmXNf9bqZ2DXJ5B7GKY8VHtVbVsL7gU3vAvZB1FLf8W0fmH06taJ7x+axoOX92PpjkziHlvOoZPOWbawssrKc1/v55Z3E4kI9uOreyadFdirDYsKZuk9ExkSEcTp0kpuTIhi0eZjXP3qelKy2sf0z+dLa833h7K49vUf+Pk7W8gpLue564aw6rdTuWlMLwnswm7u1XL3DYa81PN7T+Z28PSG8MZHxBAzxQyJrCgBix+VVVaW7shkWv8wgjpZIDsZvn/RlH0iv+mhjIvmwgHbDA4h/WDkzedXT0eLnQLTn4IVv4d1C+AiM3PEL6fGkV1UxjvrU7n57c189ssJ9Ajya7Nq5RaX88sPt7LxcC5zx0TxxKxB52yFhwX48vGd46m0aiyeHswY0p3fLE5i1t/X8cw1Q7h6RESb1f1CbTycw4v/O8CW1Dx6BvnyzDVDuH5UpAxnFC3iXsHdLxiOn2fLPTMJwgY2fal/3DTY+KoZEhl/KZuO5JJdVMbVI2yt9qQPa8s+FQyPHKsdc18t60BtYL9nGwT3bvt0TGMm3GP6E1Y+bf52+WmoQ//jiZO7uWfET/j7Xj/ef+N7fnXZUDr3mwqdWnf44f4Tp7ntvUROFZbxlxuG2d1ZrZTC4ml+VC/uF8by+y7i3kXbuX9xEhsP5/DErEH4ebffFu/+E6d57r/7WX0gi+6BvvxxziB+NDpKWunigrSDCONAvkHnl3PX2gS3Qdc0XSZ6Elg6wcFvIP5Svt59Aj+LJ1P6hpmLn3Z9amZvzEg05Z/tBfduN8Moq/3wd/CwwG3ftt5Y9pZQCma/ArlHYNmvz3qp675/8YQCSoAv67zQeyJ4+UDEKLj4dw676OqbPSf49eIk/H28+PjO8Qy/gLH3PYL8WHT7OP664iCvrUlh85FcFtw4vN2N58/ML+GvKw7y2bZ0Any8eOSK/syfEO3y/QWifXCv8z3fYDPlb1WlfeXzjpgcfY/hTZex+ELsVDj4DdYqK9/sOcHUfqGmJZi+GQqOwZjb4Rfrat/z8ghzxWvmdlh6D+z4CEbdDD3P8TnO4t0J5i6C/leZ248/hgeSYeL90GM4pwP7ckLXmWXz6HpIWQVrXzBnKqnrzI9kC2mt+fvKQ9z5wVbiw/z56p5JFxTYq3l5evDQjP78+7axlFRUce3rP/C3bw9R2Q7GxheUVPDsf/dz8V/WsDQpk9smxbD2oYv5xZQ4CezCYdyr5V59lWpJHviHNl8+0zbEseeIc5frezlwlI/4AAAa+klEQVQcWM6+XZs5VVjGjMG2qQZ2fgxeftD/SjPZ2MOp8Fy0ee2dK87ex/i77T2KthfQHW768Oxtlz4FQCCwbtdxnv54PaqqlJcv9mZ0ZCdzprJuAbx7pSk/62/Qd4YZBeRhX5uhvNLKI5/tZMn2DK4e3pNnrxvq8OA2oU8IX98/mSeX7mHBtwdZdeAUC340jNjQC1wlqwWq5yJ6ccVBCkoquGZ4BL+5rC+RXVo4978Q5+BeLffO3cz9mWz7yh9PMp2pYQPPXS7erFv61SfvAHBx/zCz4tOez82FQdXTDft1gceOm9EodcVefHaaxsXMHNKDbx+/mp5Rscxd7c8KPRqmPwm/PVA7NcNX98GL/eDpLrDiCTPsE+BMrlmXtp7C0gpueXcLS7Zn8JtL+7LgxuGt1moN8rOw4MbhvPLjEaRmF3Ply+t4d/2RNr0qd0NKDlf9fR2//3IPA7oHsuyeSfz1xuES2EWrUfoCTqkvREJCgk5MTHTsTo+shfdmwc1f1Y5RP5f3Zpu0zJ2NTzFQ1/HnRpNeDHutvbnZa0XtC3MXN5w6oLwYnrF1uN6zDQJ7gqXtRpy0lsLSCn7y1mb2Zhbwk3G96RsewE2jo1DWSlj+AGx99+w3eHiB1ZYi6z0JRt8Kg67hZGEZ89/ZwqGThfz52iHckBDV4LNay4mCUh7+bCffHcxidHQX7rukL1lFpUyICyG8FZYiTM87w5+X7+c/u44TEezH41cOYMbg7jJFgGgxpdRWrXVCs+XcKrif3Auvj4fr34HB19Zu19qkauqO9tAano+BAbNg9t+b3fWnf/kl1xb+Gw9V7+/1eNaFL6rhQgpKKpj35kZ2Z5jW+C0TY3j8ygF4VF/wVHjSDB1dtwCsFQ3fP+inzEqeTWVJAX+eN4Up/Vo46doF0Frz6dZ0/rhsL6dLzY+Pv48XD1/Rn3ljetUeywUorajije9SeH1NCkrBXVP6cOeUWMmpiwtmb3B3r5x75xBzf6be8q2HVsDieXBvkplAC0zaoCQPwgY1u9vTpRW8kzeE6y31AvttKztUYAeT4vjXrWP5bFsGB06c5u31RygsreDP1w4xE6gFhMO035lbfhoERoCugsztnP72BYL2fMBaPjAJwUVA1DiY8wp069M60x03QinFDQlRTO4bypvfHyY0wIfvDmbx+y928/svdnPrpBhG9Apm8ZY0pvUPI8DXwuxhPe0eb74+OZvffb6L1JwzXDm0B4/NHEBEsOufuQnX4l7B3a8roKA46+ztJ3dBVTmc2FUb3KvXSw0b0Oxu1x3KZk9VFKVdo/Ety4F7EqHopJmjpQMK7uTNrZNi0FrTM9iPl749RFFZJS/dNBwfL09KyqvYeCSHKfGRtlawB1ut8cw/ehtzveJ5rPJVMzTUWmHm9XnF1ggZeiMk3Ao9hrZJGis80JffXWn6W26/KJYl2zJ4aeVB3lp3BAAfLw++P2T6bxasOMjF/UO5a2qfJgN1TlEZf/rPPpZszyC6Wyf+detYJsWHtPpxCNEY9wrunl6mU/O758zqRk/axrwXpJv77IO1+fGs/ebejuC+9mAWAb4WLFc+Z+Z9D+hubh2cUor7p/clwNfCH5ftJeONDfQJ9WfJ9gwAZgzqzoIbh7M9LY/b3kskLMCHm2//PQTbFiepLIfCTPjyV5D6PexcbG4+QTDkOpj6qOmsboNAr5TiulGRXDcqkqS0fHZlFHDdyAhW7D1JWYWV179L4V8bj/FJYjp3To7lzilxeHoovDwUnh6KT7am88zyfRSXVXLPtD7cfXEfScEIp3Kv4A7QORRKcgFt5kw/+LVJDwDkHKotd2ovdOpmyjdjfUo242K74dm/2TRXh3TrpBgCfL146NOd7Ew3VwhfPyqSz7alc81r6zmSXUxvW0s2rG6npZc3dImG+bZ5efZ9Bds/hCPfQeLb5gYmZdNrvCkbf2mrnzENjwquGWs/Z7g507tuVCRHc4p56dtDvLwqmZdXJTd43+joLjxzzRDiw1uwQLoQDuaGwT0Esg+YxwunQGUp+Nta2dl1g/t+MwSymTxvWu4Z0nJLuHVizDnLdXQ/SogiLrQzm47k8vMJMfh5e3L5oO7cu2g78eH+vH/L2Oanpx0wy9zKiuDAf01wz9oHOcnmBrDqjzD5QRj3y1afDqEuTw9FbKg/L88dwc0TevOP7w5TWmmluKySAF8vrhzSg+tGRjqkM1YIR3C/4N6pW+3jylJzX3TC3FcHd63h1D67FqNen2xyrhP7SO60OaN6d2VU79qAe+nAcL5/+GICfL3Ob54UH38YeoO5gRknn7YJkleaieHWvmBuPYabBc0nP2g6xu28eOpCjerdlYU/k2X+RPvmfsG9qTSLf3cT5M/kwluXQXmhXfn29Sk5hAb40Ces7a9odAch/j4XvhPfQJOOib/UPN+/HNY+b6Z3OJ4Ee22T3/iHQ2h/cxs1H0LizVj78mKoOGM6wQMjTL9MZZmZWkIIN+V+wd2vS+Pb46bBjn+b1nt17j0k/py70lqzISWbiX1C5KKT9qT/THMryDBTMf/wMuxfZoJ30UmTs9/8D/v2Neha8wPQJdrMEdQ5tM2GZArRmtwvuHtaGt9eHdyrh0DCuScMAw6eLCK7qJyJcZKSaZeqh7XOftnctDbDYNMT4dD/zCRnZUXmBz8gHLz9zRXJOSlmqoriHNizpHZ/a583951DTe7f0gkGXweh/Zy3sIoQLeR+wd2jidxu9ESwdDYXNAFc9ZI53T+HTUfMxVDj47qds5xoJ5QC/7Dalr09Tu0zKZuqStj0uulozz9WO1JnwyvmPnywWWoxYhSEDzILrlgrmj5TFMLJ3C+4D74O1r4IlSXmeacQs8i1f3cIHwjJ35rtdsyrnpiaR3igD5Fd5OpCt1W336XuUosZ28y89bs/M9M3pyfCyd0N589BmVy/X7DpzA8bYCaJ6znCtPx7DDM5fy/f9rFAi+gw3O9fW9dYePwEbH3PnIL3nmA63Tw8TOsrfYutXPPBfevRPBJ6d5V8e0cUMdLch9ump6iqNBdc7VsGBWlmgRO0yftXlZl0UP4xkwpqTHBv82/R0snMod8t3lyPYa00Pw5evmZKjOqpM8IGwsA5Ziio8jB9AlUVbTr8U7g2u4K7UmoG8DfAE3hTa/1sE+WuBz4BRmutHTwr2HkaVWeN0kjbxUfdB5t7Lz8IaLjgcl0nCkrJyC/h1kkyvl1gWt3BvWD8L5suY7WaK5hzUiBts2lcZGw1qaKTe2DHovP7zCW3nf1cecKAqyD+cnNW0WN4bV+CtbLp/iaA4zvNj0lOivkR6tQVKkrN+738IGqMOfvw8jP78pHRYa6u2eCulPIEXgUuBdKBLUqppVrrvfXKBQD3Aptao6IOUb0IdtfYZsdEbz2aB8Co3pJTFXby8DA5+MiE2gZFXZXlJk9fUQIndoJPoFkaMi8VSvIhKNJ0Env5mmB84D8Q0NP8sOSnmffuXVo79LMu5WGmaigvhu5DzbagCNOhfGpf7bUe9uoWD11jzBmuT4AZheThZa4WDhtgzoI7dW26j0s4nT0t9zFAstb6MIBS6iNgDrC3Xrk/As8DDzi0ho4UbluUo1vzC2ckHs3F1+LBwJ7n7nQVwm5e3oC3GXkTN612e2NDcuOnm1t9M54z6xZUlZsrsc/kmov1SvLNIjXeAaZvQGs4vsMsRhM+CEb8xJx5BEaYlFP+MZMOyku1jSA6ZAK41Wpa7hmJ5qzj0P+aPh7fYPOD5Btk9h1/qXns6W0uKqtOIUla0ynsCe4RQFqd5+nA2LoFlFIjgCit9TKlVPsN7j4BMPo2iL6o2aLbjuYxLDIYi6d7LVYlXJx3p4aLwzRF66YDa3XgDaxOTzaxz9ICc8bROQSsVWbd4cztJr2TtQ9yD5uJ+TK2nZ12Up4mTVRZZs5QfALMj4zFz6SUfAJMP0LYAOgxAtC1U3YLh7AnuDf2r6NmYnOllAewAJjf7I6UugO4A6BXr17NlG4lV77YbJGS8ir2ZJ7mzimuuzSeEA5pMfsG1T729DJnGY2daVRVmMXSi7NNyz9rP+SmmDOJstNm4frjO8yPhbWy0YVcUJ6mX8w32Ixmi55kUqjdh7XZ1BLuxJ7gng7UXQctEsis8zwAGAyssY0q6Q4sVUrNrt+pqrVeCCwEsxLTBdS7Ve3OLKDSqhkRJfl2IeziaYG4i+0vb7Wa4J99yPQ/aKsZeXRytxmJlLap9loD5WE6jbvGmrmEIkebNFB1f0BQpCkj6Z+z2BPctwDxSqkYIAO4Cfhx9Yta6wKg5nxKKbUGeMDpo2UuwC7btLVDI4OaKSmEaBEPj9qzgMYuOKsohfTNkHfUDF8uLTD9BKnrGu9Q7tTN/ABUlpkrioOjzBxDFaWmj61bvBkh59/8FN/uotngrrWuVEr9CvgGMxTyba31HqXU00Ci1nppa1eyre3OLCAswOfsuceFEG3H4msWuY8BRv60drvWphM4/5hZfKfijGnxF9uWzfT0MWcE6YlQUdxwvz5BZt9evuaHxdLJvK9bHHQOM1NPeFpq8/+dQszCPP7hpqPYhS5Es6umWuvlwPJ62/7QRNmpF14t59qdUcCQCGm1C9HuKGUbohkDsVOaLmetMukdvy5mPqn8o+ZHIPugGV1UUWI6hUvzTav/6A9mrd/m+Aabzw7oYTqIvXzMD4B/uBl66uljOqm9A5zeT+A6P0Nt5Ex5Jcmnipgx+NwXOQkh2jEPTwjpYx7HXAQ0P0LODCfNgbJCc6ssNfeFJ8ww04pSc4aQd9T0DRxY3vS+vP1NHTqHQpcY8yMQ3NsMg+3UFfpebvoQWpEE93r2HS/EqpGWuxAdjV+wudmrrNDk+IuzzTUC+cfMVBRFWWaqivIzpkz2ATOa6ODXZqQQmCGhEtzb1u4M05kqwV0IcU4+AebWOQTC+jdfvvoK5ZL8NplCWoJ7PbsyCgjx9yY80AErCAkhRLW6Vyi3AbkyoJ7dGQUMjgiSmSCFEC5NgnsdZZVVHDpVxOCekpIRQrg2Ce51pJwqpsqq6d8jwNlVEUKICyLBvY6DJwsB6BcuwV0I4dokuNdx4GQhFk9FdIgshiyEcG0S3Os4eKKQuFB/meZXCOHyJIrVceBkIX0lJSOEcAMS3G2KyipJzyuhX3cJ7kII1yfB3aa6M1Va7kIIdyDB3ebgCRkpI4RwHxLcbQ6dKsLX4kFkFz9nV0UIIS6YBHebw1lFxIT44+Eh0w4IIVyfBHebI9nFxMr4diGEm5DgDpRXWknLKyE2VIK7EMI9SHAHjuWeocqqiZGWuxDCTUhwx6RkAAnuQgi3IcEd05kKEBvi7+SaCCGEY0hwx7Tcu3X2JqiTxdlVEUIIh5DgDhzOLpaUjBDCrXTo4P7GdylEP/IfNh/JlZkghRBupUNHtOe/3l/zWCYME0K4kw4Z3LXWfLDxKF07+9Rse2zmACfWSAghHMvL2RVwhvS8En7/xW4AgjtZuGtKHN5eHfJ3TgjhpjpkRMvIL6l5PH9CNHdOiXNibYQQwvE6XHDfmZ7PG9+l1DyP6tLJibURQojW0eHSMvPe3ERhaSUAn901gRFRwU6ukRBCOJ5bt9zTcs/w0eZjZ23zrjPkcVTvLjLFrxDCLdkV3JVSM5RSB5RSyUqpRxp5/TdKqb1KqZ1KqZVKqd6Or+r5+zgxjUeW7OJoTnHNtgDfDneyIoTogJoN7kopT+BV4ApgIDBXKTWwXrHtQILWeijwKfC8oyvaEnlnygFYtf8UYIZAnjxdxjUjIvjhkWnOrJoQQrQqe1ruY4BkrfVhrXU58BEwp24BrfVqrfUZ29ONQKRjq3luGw/nYLXqBtsLSkxuvTq4p+eVUFJRxcAegfQMluX0hBDuy57gHgGk1XmebtvWlFuB/zb2glLqDqVUolIqMSsry/5ansPmI7nctHAjr65OPmt7lVWTb2u5bzqcS3FZJT9/dwsAPha37moQQgi7gntjPY4Nm8mAUuonQALwQmOva60Xaq0TtNYJoaGh9tfyHEorqgBYn5Jds+2H5GziHltOYmoeAb5elFdZWXMgqybffnG/MId8thBCtFf2BPd0IKrO80ggs34hpdR04HfAbK11mWOq1zwvT/Pbc7ygtGbb1qN5AJRUVDGlbyhhAT58kZSBv48XI3oFE9VVxrYLIdybPcF9CxCvlIpRSnkDNwFL6xZQSo0A/oEJ7KccX82mlVdaATieXxvc/euMiOna2ZtZw3qy5sApDp0sItTfp8E+hBDC3TQb3LXWlcCvgG+AfcDHWus9SqmnlVKzbcVeAPyBT5RSSUqppU3szuGqg3t5lbVmW5HtIiWAYD8L14yIoKJKc+J0KSEBEtyFEO7PrkHfWuvlwPJ62/5Q5/F0B9fLbhVVten/0ooqfC2evLjiYM22oE7eDOoZSN9wfw6eLCJEWu5CiA7ApYeNVFZZuWfRtprnablnznr92pERTI4PQSnF7GE9Aaio08IXQgh35dKXa6bmFFN3ePuBk4VE25bLC/Kz8NcfDa957dZJsRwvKOXHY3q1dTWFEKLNuXTL3cvj7OrvzjhNcZnJt997SfxZr/l5e/Kna4bISBkhRIfg0sG90lqbYunVtRN7MgtqZnwM8HHpkxIhhLggLh3c63amjuwVzJ7M0/zhS7PCklyFKoToyFw6AtbtHB3Rqwu5xeWsPmCmNVBKpvIVQnRcLh7cTcv9rqlxjI3tetZrVw7p4YwqCSFEu+DSwb3S1nKf1CeEvmEBdOvsDUDfcH88ZREOIUQH5trB3TYO0stD4eGhGBfXDQA/b+lMFUJ0bC4d3Ktz7l62pfMu6hMCQHq9i5mEEKKjcfHgblru1euiTh8YDkBOcbnT6iSEEO2By+YvtNY8+OkOoHba3xB/H342vjcDegQ6s2pCCOF0Lhvcyyqt5J+pAMDiWdt5+vScwc6qkhBCtBsum5ax6toLmOpPQyCEEB2dy0bFyjozhnl5yrBHIYSoy2WDu9Xa6DKuQgghcOHgXt1y97N4EhHs5+TaCCFE++Kywb265f74VQNkHhkhhKjHZYN7dcvdUwK7EEI04LLBvao6uMscMkII0YDLB3cZKSOEEA25bHCvTst4SFpGCCEacNngXn0Rk1zAJIQQDblsZKysqs65O7kiQgjRDrlsaKxuuXtKy10IIRpw2chYMxTSZY9ACCFaj8uGxtqhkC57CEII0WpcNjJWyUVMQgjRJNcP7nIRkxBCNODywV0uYhJCiIbsCu5KqRlKqQNKqWSl1CONvO6jlFpse32TUira0RWtr0rLRUxCCNGUZoO7UsoTeBW4AhgIzFVKDaxX7FYgT2vdB1gAPOfoitZXZbUC4CVpGSGEaMCelvsYIFlrfVhrXQ58BMypV2YO8J7t8afAJaqV5+GtMrFdcu5CCNEIexbIjgDS6jxPB8Y2VUZrXamUKgC6AdmOqGRdH29J45/fH6aorBKQtIwQQjTGnuDeWPSsv8adPWVQSt0B3AHQq1cvOz66oeBOFuLD/QGY6udNXFjnFu1HCCHcmT3BPR2IqvM8Eshsoky6UsoLCAJy6+9Ia70QWAiQkJDQokVQLxvUncsGdW/JW4UQosOwJ+e+BYhXSsUopbyBm4Cl9cosBW62Pb4eWKW1lhWshRDCSZptudty6L8CvgE8gbe11nuUUk8DiVrrpcBbwAdKqWRMi/2m1qy0EEKIc7MnLYPWejmwvN62P9R5XArc4NiqCSGEaCmXvUJVCCFE0yS4CyGEG5LgLoQQbkiCuxBCuCEJ7kII4YaUs4ajK6WygKMtfHsIrTC1QTsnx9wxyDF3DBdyzL211qHNFXJacL8QSqlErXWCs+vRluSYOwY55o6hLY5Z0jJCCOGGJLgLIYQbctXgvtDZFXACOeaOQY65Y2j1Y3bJnLsQQohzc9WWuxBCiHNwueDe3GLdrkopFaWUWq2U2qeU2qOUus+2vatSaoVS6pDtvottu1JKvWz7O+xUSo107hG0jFLKUym1XSm1zPY8xrbI+iHbouvetu1tvgh7a1BKBSulPlVK7bd91+M7wHf8a9u/6d1KqUVKKV93/J6VUm8rpU4ppXbX2Xbe361S6mZb+UNKqZsb+yx7uFRwt3OxbldVCfxWaz0AGAfcbTu2R4CVWut4YKXtOZi/QbztdgfwettX2SHuA/bVef4csMB2vHmYxdfBCYuwt5K/AV9rrfsDwzDH7rbfsVIqArgXSNBaD8ZMG34T7vk9vwvMqLftvL5bpVRX4AnMUqZjgCeqfxDOm9baZW7AeOCbOs8fBR51dr1a6Vi/BC4FDgA9bNt6AAdsj/8BzK1Tvqacq9wwq3qtBKYByzDLNWYDXvW/b8x6AuNtj71s5ZSzj+E8jzcQOFK/3m7+HVevr9zV9r0tAy531+8ZiAZ2t/S7BeYC/6iz/axy53NzqZY7jS/WHeGkurQa26noCGATEK61Pg5guw+zFXOHv8VLwEOA1fa8G5Cvta60Pa97TGctwg5UL8LuSmKBLOAdWyrqTaVUZ9z4O9ZaZwB/AY4BxzHf21bc+3uu63y/W4d9564W3O1aiNuVKaX8gc+A+7XWp89VtJFtLvO3UEpdBZzSWm+tu7mRotqO11yFFzASeF1rPQIopvY0vTEuf8y2lMIcIAboCXTGpCTqc6fv2R5NHafDjt/Vgrs9i3W7LKWUBRPYP9RaL7FtPqmU6mF7vQdwyrbd1f8WE4HZSqlU4CNMauYlINi2yDqcfUw1x3uuRdjbuXQgXWu9yfb8U0ywd9fvGGA6cERrnaW1rgCWABNw7++5rvP9bh32nbtacLdnsW6XpJRSmLVo92mt/1rnpbqLj9+MycVXb/+Zrdd9HFBQffrnCrTWj2qtI7XW0ZjvcZXWeh6wGrPIOjQ8XpdehF1rfQJIU0r1s226BNiLm37HNseAcUqpTrZ/49XH7Lbfcz3n+91+A1ymlOpiO+u5zLbt/Dm7A6IFHRYzgYNACvA7Z9fHgcc1CXP6tRNIst1mYvKNK4FDtvuutvIKM3IoBdiFGY3g9ONo4bFPBZbZHscCm4Fk4BPAx7bd1/Y82fZ6rLPr3cJjHQ4k2r7nL4Au7v4dA08B+4HdwAeAjzt+z8AiTL9CBaYFfmtLvlvgFtvxJwM/b2l95ApVIYRwQ66WlhFCCGEHCe5CCOGGJLgLIYQbkuAuhBBuSIK7EEK4IQnuQgjhhiS4CyGEG5LgLoQQbuj/Ac+Rqs/X4edWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "class bandit_arm_v4():\n",
    "    def __init__(self, mean: float, std: float):\n",
    "        # Start equally\n",
    "        self.mean = 0\n",
    "        self.std = 0.1\n",
    "        \n",
    "        # Random walk parameters\n",
    "        self.mean_change = mean\n",
    "        self.std_change = std\n",
    "        \n",
    "        self.distribution = np.random.normal\n",
    "        \n",
    "    def random_walk(self):\n",
    "        self.mean += self.distribution(loc=self.mean_change,\n",
    "                                       scale=self.std_change)\n",
    "        \n",
    "    def r(self):\n",
    "        while True:\n",
    "            self.random_walk()\n",
    "            yield self.distribution(loc=self.mean, \n",
    "                                    scale=self.std)\n",
    "\n",
    "class bandit_v4():\n",
    "    def __init__(self, k: int, c: int):\n",
    "        self.A = self._create_arms(k)\n",
    "        self.R = dict()\n",
    "        self.n = 0\n",
    "        self.c = c\n",
    "        self.chosen = np.zeros(k)\n",
    "        self.total_reward = 0\n",
    "        self.cum_rewards = []\n",
    "        self.is_optimal = 0\n",
    "        self.cum_optimal = []\n",
    "        \n",
    "    def game(self, T: int):\n",
    "        self.R = self._init_rewards()\n",
    "        self.total_reward = 0\n",
    "        self.n = 0\n",
    "        for i in range(T):\n",
    "            self.n += 1\n",
    "            Rt = self.play()\n",
    "            self.total_reward += Rt\n",
    "            self.cum_rewards.append(self.total_reward)\n",
    "            self.cum_optimal.append(self.is_optimal / (i+1))\n",
    "        return self.total_reward\n",
    "            \n",
    "    def play(self):\n",
    "        a = self._choose_action()\n",
    "        self.is_optimal += self._is_optimal(a)\n",
    "        return self._do_action(a)\n",
    "        \n",
    "    def _create_arms(self, k: int):\n",
    "        return [bandit_arm_v4(0, 1) for a in range(k)]\n",
    "    \n",
    "    def _get_optimal_hand(self):\n",
    "        hands_rewards = list(map(lambda arm: arm.mean, self.A))\n",
    "        #print(f'_get_optimal_hand:\\t{hands_rewards})')\n",
    "        return np.argmax(hands_rewards)\n",
    "    \n",
    "    def _is_optimal(self, chosen_hand: int):\n",
    "        return 1 if chosen_hand == self._get_optimal_hand() else 0\n",
    "    \n",
    "    def _init_rewards(self):\n",
    "        rewards = dict()\n",
    "        for arm in range(len(self.A)):\n",
    "            rewards[arm] = 0\n",
    "        return rewards\n",
    "    \n",
    "    def _choose_action(self):\n",
    "        Qt_upper = [self._calc_bound(a, True) for a in range(len(self.A))]\n",
    "        Qt_lower = [self._calc_bound(a, False) for a in range(len(self.A))]\n",
    "        \n",
    "        # Eliminate bad actions by bound-checking\n",
    "        possible_arms = [self._test_arm_bounds(a, Qt_lower) for a in Qt_upper]\n",
    "        \n",
    "        # Return random arm from possible arms\n",
    "        chosen_arm = np.random.choice(range(len(possible_arms)))\n",
    "        #print(f'**{self.n}**\\nQt_Upper: {Qt_upper}\\nQt_Lower: {Qt_lower}\\nPossibles: {possible_arms}')\n",
    "        while (possible_arms[chosen_arm] == False):\n",
    "            chosen_arm = np.random.choice(range(len(possible_arms)))\n",
    "        \n",
    "        return chosen_arm\n",
    "    \n",
    "    def _test_arm_bounds(self, arm_upper_bound: float, arms_lower_bounds: list):\n",
    "        arm_test = [arm_upper_bound < arm_lower_bound for arm_lower_bound in arms_lower_bounds]\n",
    "        #print(f'ub: {arm_upper_bound}\\tlbs: {arms_lower_bounds}')\n",
    "        if True in arm_test:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def _Q(self, a):\n",
    "        return self.R[a] / self.chosen[a]\n",
    "    \n",
    "    def _calc_bound(self, a: int, upper: bool):\n",
    "        N_t = self.chosen[a]\n",
    "        calc_a = self._Q(a)\n",
    "        calc_b = self.c * np.sqrt(np.log(self.n) / N_t)\n",
    "        \n",
    "        if upper:\n",
    "            bound = calc_a + calc_b\n",
    "        else:\n",
    "            bound = calc_a - calc_b\n",
    "        return bound\n",
    "    \n",
    "    def _do_action(self, a):\n",
    "        Rt = next(self.A[a].r())\n",
    "        self.R[a] += Rt\n",
    "        self.chosen[a] += 1\n",
    "        return Rt\n",
    "\n",
    "k = 5\n",
    "epsilon = 0.1\n",
    "c = 2\n",
    "T = 1000\n",
    "bandit_UCB = bandit_v4(k, c)\n",
    "bandit_dynamic_alpha = bandit_v3(k, epsilon, False)\n",
    "\n",
    "def run_simulation(bandit):\n",
    "    total_reward = bandit.game(T)\n",
    "    optimal_reward = np.max(list(map(lambda x: x.mean, bandit.A)))\n",
    "    total_optimal_reward = T * optimal_reward\n",
    "    print('K-Arm Bandit with UCB Q Function')\n",
    "    print(f'Total Reward: {total_reward}')\n",
    "    print(f'Optimal Reward: {optimal_reward}\\tOptimal Total Reward: {total_optimal_reward}')\n",
    "    print(f'Regret: {total_optimal_reward - total_reward} / {100*(1-(total_reward/total_optimal_reward)):.2f}%')\n",
    "    for i in bandit.R:\n",
    "        print(f'arm {i}:\\t # chosen: {bandit.chosen[i]:0.0f}\\t mean: {bandit.R[i]}\\t true: {bandit.A[i].mean}\\t delta: {abs(bandit.A[i].mean-bandit.R[i])}\\t delta %: {abs(1-bandit.A[i].mean/np.mean(bandit.R[i]))}\\n')\n",
    "    return bandit\n",
    "\n",
    "print('**UCB**')\n",
    "bc = run_simulation(bandit_UCB)\n",
    "bd = run_simulation(bandit_dynamic_alpha)\n",
    "\n",
    "plt.plot(bc.cum_optimal, label='UCB')\n",
    "plt.plot(bd.cum_optimal, label='Const Alpha')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.8**: UCB Spikes In Figure 2.4 the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps. Hint: if c = 1, then the spike is less prominent.\n",
    "\n",
    "**Answer**: The spike happens due to the algorithm beginning to eliminate non-profitable arms.\n",
    "When the algorithm starts it takes about K steps (or a bit more) until all hands are tested and base bounds are calculated.\n",
    "It then takes a few more turns until the bounds starts to tighten and the worst hands are being knocked off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient bandit\n",
    "\n",
    "All the methods up to this one were based on estimating the action value, and using the estimate to choose the proper hand.   In this variation we will try to choose hands by a numerical preference $H_t(a)$ (The larger, the more likely we are to choose it).   $H_t(a)$ doesn't have any interpertation in terms of reward but instead it is relative to the other hands and determined by a soft-max function (Gibbs / Boltzman distribution).\n",
    "\n",
    "$Pr(A_t=a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^{k}{e^{H_t(b)}}} = \\pi_t(a)$\n",
    "\n",
    "$H_t(a)$ is based on **Stochastic gradient ascent**:\n",
    "\n",
    "$H_{t+1}(A_t) = H_t(A) + \\alpha(R-\\hat{R_t})(1-\\pi_t(A_t))$ || for $a=A_t$\n",
    "\n",
    "$H_{t+1}(a) = H_{t}(a) - \\alpha(R-\\hat{R_t})\\pi_t(a)$ || for $a \\neq A_t$\n",
    "\n",
    "* $\\alpha$: Step-size parameter\n",
    "* $\\pi_t(a)$: Soft-Max distribution of the wanted class by $H_t(a)$\n",
    "* $\\hat{R}$: Aerege of results up to (including) the current sample\n",
    "\n",
    "Every round we will check the reward given by $a = A_t$ (The chosen action).  \n",
    "If the reward is greater then the current baseline $(\\hat{R_t})$ then we will increase it's chances of being selected $(\\pi_t(A_t))$ by an amount regulated by $\\alpha$ and decrease the chance of the other hands to be selected by that same principle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Gradient**\n",
      "K-Arm Bandit with Gradient Q Function\n",
      "Total Reward: 1164.7685693818607\n",
      "Optimal Reward: 10.851845183155675\tOptimal Total Reward: 10851.845183155674\n",
      "Regret: 9687.076613773814 / 89.27%\n",
      "arm 0:\t # chosen: 8\t mean: -14.039141054372998\t true: -4.750143316335848\t delta: 9.28899773803715\t delta %: 0.6616500042318298\n",
      "\n",
      "arm 1:\t # chosen: 567\t mean: 61.74910556555568\t true: -16.124176588780706\t delta: 77.87328215433638\t delta %: 1.2611240509656054\n",
      "\n",
      "arm 2:\t # chosen: 29\t mean: -22.41357650744653\t true: -6.192480227569575\t delta: 16.221096279876953\t delta %: 0.723717443063483\n",
      "\n",
      "arm 3:\t # chosen: 158\t mean: 571.5771828083855\t true: 10.851845183155675\t delta: 560.7253376252298\t delta %: 0.9810142085626368\n",
      "\n",
      "arm 4:\t # chosen: 238\t mean: 567.8949985697399\t true: -1.2663898081339242\t delta: 569.1613883778738\t delta %: 1.0022299717576724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/orz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/orz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Arm Bandit with Gradient Q Function\n",
      "Total Reward: 3353.987515839515\n",
      "Optimal Reward: 6.497702326580386\tOptimal Total Reward: 6497.702326580386\n",
      "Regret: 3143.7148107408716 / 48.38%\n",
      "arm 0:\t # chosen: 138\t mean: 24.63265224621682\t true: 2.567945453988587\t delta: 22.064706792228236\t delta %: 0.8957503468030739\n",
      "\n",
      "arm 1:\t # chosen: 183\t mean: -4.146961698460533\t true: -11.304231412677977\t delta: 7.157269714217444\t delta %: 1.7259068770455297\n",
      "\n",
      "arm 2:\t # chosen: 368\t mean: 2754.093445467978\t true: 6.497702326580386\t delta: 2747.5957431413976\t delta %: 0.9976407110160794\n",
      "\n",
      "arm 3:\t # chosen: 40\t mean: -82.13214479128398\t true: -7.713224304264221\t delta: 74.41892048701976\t delta %: 0.9060876308068511\n",
      "\n",
      "arm 4:\t # chosen: 271\t mean: 661.5405246150643\t true: -8.714789904121861\t delta: 670.2553145191862\t delta %: 1.0131734785396447\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import scipy\n",
    "\n",
    "class bandit_arm_v4():\n",
    "    def __init__(self, mean: float, std: float):\n",
    "        # Start equally\n",
    "        self.mean = 0\n",
    "        self.std = 0.1\n",
    "        \n",
    "        # Random walk parameters\n",
    "        self.mean_change = mean\n",
    "        self.std_change = std\n",
    "        \n",
    "        self.distribution = np.random.normal\n",
    "        \n",
    "    def random_walk(self):\n",
    "        self.mean += self.distribution(loc=self.mean_change,\n",
    "                                       scale=self.std_change)\n",
    "        \n",
    "    def r(self):\n",
    "        while True:\n",
    "            self.random_walk()\n",
    "            yield self.distribution(loc=self.mean, \n",
    "                                    scale=self.std)\n",
    "\n",
    "class bandit_gradient():\n",
    "    def __init__(self, k: int, alpha: float):\n",
    "        self.A = self._create_arms(k)\n",
    "        self.R = {}\n",
    "        self.R_sum = 0\n",
    "        self.current_r = 0\n",
    "        self.n = 0\n",
    "        self.alpha = alpha\n",
    "        self.H = np.array([1/k]*k)\n",
    "        self.softmaxes = np.array([1/k]*k)\n",
    "        self.chosen = np.zeros(k)\n",
    "        self.total_reward = 0\n",
    "        self.cum_rewards = []\n",
    "        self.is_optimal = 0\n",
    "        self.cum_optimal = []\n",
    "        \n",
    "    def game(self, T: int):\n",
    "        self.R = self._init_rewards()\n",
    "        self.total_reward = 0\n",
    "        self.n = 0\n",
    "        for i in range(T):\n",
    "            self.n += 1\n",
    "            Rt = self.play()\n",
    "            self.total_reward += Rt\n",
    "            self.cum_rewards.append(self.total_reward)\n",
    "            self.cum_optimal.append(self.is_optimal / (i+1))\n",
    "        return self.total_reward\n",
    "            \n",
    "    def play(self):\n",
    "        a = self._choose_action()\n",
    "        self.is_optimal += self._is_optimal(a)\n",
    "        return self._do_action(a)\n",
    "        \n",
    "    def _create_arms(self, k: int):\n",
    "        return [bandit_arm_v4(0, 1) for a in range(k)]\n",
    "    \n",
    "    def _get_optimal_hand(self):\n",
    "        hands_rewards = list(map(lambda arm: arm.mean, self.A))\n",
    "        #print(f'_get_optimal_hand:\\t{hands_rewards})')\n",
    "        return np.argmax(hands_rewards)\n",
    "    \n",
    "    def _is_optimal(self, chosen_hand: int):\n",
    "        return 1 if chosen_hand == self._get_optimal_hand() else 0\n",
    "    \n",
    "    def _init_rewards(self):\n",
    "        rewards = dict()\n",
    "        for arm in range(len(self.A)):\n",
    "            rewards[arm] = 0\n",
    "        return rewards\n",
    "    \n",
    "    def _choose_action(self):\n",
    "        # Select arm by calculated Pi(a)\n",
    "        chosen_arm = np.random.choice(range(k), p=self.softmaxes)\n",
    "        #print(f'**{self.n}**\\nQt_Upper: {Qt_upper}\\nQt_Lower: {Qt_lower}\\nPossibles: {possible_arms}')\n",
    "        return chosen_arm\n",
    "    \n",
    "    def _pi(self, a):\n",
    "        softmax_actions = scipy.special.softmax(self.H)\n",
    "        a_t_softmax = softmax_actions[a]\n",
    "        return a_t_softmax\n",
    "    \n",
    "    def _H(self, a: int, is_a_t: bool):\n",
    "        pi = (1-self._pi(a)) if is_a_t else self._pi(a)\n",
    "        update = self.alpha*(self.current_r-self.R_hat)\n",
    "        h_t = self.H[a] + (update * pi)\n",
    "#         print(f'H({a}| (At == {is_a_t})): {h_t}')\n",
    "        return h_t\n",
    "    \n",
    "    def _do_action(self, a):\n",
    "        Rt = next(self.A[a].r())\n",
    "        self.current_r = Rt\n",
    "        self.R[a] += Rt\n",
    "        self.R_sum += Rt\n",
    "        self.n += 1\n",
    "        self.R_hat = self.R_sum / self.n\n",
    "        self.H = [self._H(action, action==a) for action in range(len(self.A))]\n",
    "        self.softmaxes = scipy.special.softmax(self.H)\n",
    "#         print(f'softmaxes: {self.softmaxes}')\n",
    "        self.chosen[a] += 1\n",
    "        return Rt\n",
    "\n",
    "k = 5\n",
    "epsilon = 0.4\n",
    "c = 2\n",
    "T = 1000\n",
    "bandit_UCB = bandit_v4(k, c)\n",
    "bandit_gradient = bandit_gradient(k, epsilon)\n",
    "\n",
    "def run_simulation(bandit):\n",
    "    total_reward = bandit.game(T)\n",
    "    optimal_reward = np.max(list(map(lambda x: x.mean, bandit.A)))\n",
    "    total_optimal_reward = T * optimal_reward\n",
    "    print('K-Arm Bandit with Gradient Q Function')\n",
    "    print(f'Total Reward: {total_reward}')\n",
    "    print(f'Optimal Reward: {optimal_reward}\\tOptimal Total Reward: {total_optimal_reward}')\n",
    "    print(f'Regret: {total_optimal_reward - total_reward} / {100*(1-(total_reward/total_optimal_reward)):.2f}%')\n",
    "    for i in bandit.R:\n",
    "        print(f'arm {i}:\\t # chosen: {bandit.chosen[i]:0.0f}\\t mean: {bandit.R[i]}\\t true: {bandit.A[i].mean}\\t delta: {abs(bandit.A[i].mean-bandit.R[i])}\\t delta %: {abs(1-bandit.A[i].mean/np.mean(bandit.R[i]))}\\n')\n",
    "    return bandit\n",
    "\n",
    "print('**Gradient**')\n",
    "bc = run_simulation(bandit_UCB)\n",
    "bd = run_simulation(bandit_gradient)\n",
    "\n",
    "plt.plot(bc.cum_optimal, label='UCB')\n",
    "plt.plot(bd.cum_optimal, label='Gradient')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associative search (Contextual Bandits)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.10** Suppose you face a 2-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5 (case A), and 0.9 and 0.8 with probability 0.5 (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?\n",
    "\n",
    "Case A - Unknown State:  \n",
    "$Action 1 = Pr(State A) * Action Value >> 0.5*0.1 + 0.5*0.9 = 0.5$  \n",
    "$Action 2 = Pr(State B) * Action Value >> 0.5*0.2 + 0.5*0.8 = 0.5$  \n",
    "Total = 0.5\n",
    "\n",
    "Case B - Known State:  \n",
    "$State 1 = 0.5 * 0.2$  \n",
    "$State 2 = 0.5 * 0.9$  \n",
    "Total = 0.55\n",
    "\n",
    "To achieve the expectation in the 2nd case opposed to the unknown (A) case we would need to run a different bandit experiment for each state.  \n",
    "Since we do not know the distribution of the reward for each state we would need a bandit to figure it out and choose the best action for every state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
